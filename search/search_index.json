{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"C++ API: R: Python3: Fast calculation of LD in large-scale cohorts \u00b6 Tomahawk is a machine-optimized library for computing linkage-disequilibrium from population-sized datasets. Tomahawk permits close to real-time analysis of regions-of-interest in datasets of many millions of diploid individuals on a standard laptop. All algorithms are embarrassingly parallel and have been successfully tested on chromosome-sized datasets with up to 10 million individuals. Tomahawk uniquely constructs complete haplotype/genotype contigency matrices for each comparison, perform statistical tests on the output data, and provide a framework for querying the resulting data. CLI Commands \u00b6 Command Description aggregate data rasterization framework for TWO files calc calculate linkage disequilibrium scalc calculate linkage disequilibrium for a single site concat concatenate TWO files from the same set of samples import import VCF / VCF.gz / BCF to TWK sort sort TWO file view TWO -> LD / TWO view, TWO subset and filter haplotype extract per-sample haplotype strings in FASTA /binary format decay compute LD-decay over distance","title":"Home"},{"location":"#fast-calculation-of-ld-in-large-scale-cohorts","text":"Tomahawk is a machine-optimized library for computing linkage-disequilibrium from population-sized datasets. Tomahawk permits close to real-time analysis of regions-of-interest in datasets of many millions of diploid individuals on a standard laptop. All algorithms are embarrassingly parallel and have been successfully tested on chromosome-sized datasets with up to 10 million individuals. Tomahawk uniquely constructs complete haplotype/genotype contigency matrices for each comparison, perform statistical tests on the output data, and provide a framework for querying the resulting data.","title":"Fast calculation of LD in large-scale cohorts"},{"location":"#cli-commands","text":"Command Description aggregate data rasterization framework for TWO files calc calculate linkage disequilibrium scalc calculate linkage disequilibrium for a single site concat concatenate TWO files from the same set of samples import import VCF / VCF.gz / BCF to TWK sort sort TWO file view TWO -> LD / TWO view, TWO subset and filter haplotype extract per-sample haplotype strings in FASTA /binary format decay compute LD-decay over distance","title":"CLI Commands"},{"location":"aggregation/","text":"Aggregating datasets \u00b6 Motivation \u00b6 Tomahawk generally output many millions to many hundreds of millions, or even billions, of output linkage disequilibrium (LD) associations generated from many millions of input SNVs. It is technically very challenging to visualize such large datasets. Not only because of hardware limitations such as loading all the data into memory, or directly rendering billions of data points, but also because of more practical considerations such as cramming such a vast number of data points into a finite number of pixels would result in an absolute horrendous and uninformative image. In order to get a scope of the scale this problem presents, take for example a small chromosome like chr20 with data from the 1000 Genomes Project Phase 3 (1KGP3). This data comprises of 1,733,484 diploid SNVs. Assuming we can plot the LD data for a pair of SNVs in a single pixel, a monitor would have to have the dimensions 400 x 400 meters to display this data*! Not only would the monitor have to be huge, the memory requirement for plotting this image would be around 400 GB! Here we describe methods to overcome these obstacles. * Assuming a 1920 x 1080 pixel resolution and 20\" monitor as reference Existing solutions \u00b6 There are several existing solutions for aggregating large datasets, such as Datashader for Python users. But packages like this requires us to leave the highly compressed internal binary representation of Tomahawk in order to transform two records into a form understandable by these frameworks. We have tried several of the most popular framework for aggregating datasets and found none that works in reasonable memory and is sufficiently efficient when applied to our specific use-case. Aggregation \u00b6 Aggregation is the process of reducing larger datasets to smaller ones for the purposes of displaying more data than can fit on the screen at once while maintaining the primary features of the original dataset. Tomahawk performs aggregation into regular grids (two-dimensional partitions) by applying summary statistics function on data collected in the given bins. At the moment, Tomahawk supports aggregation by Function Action Summation Sum total of the desired property Summation squared Sum total of squares of the desired property Mean Mean of the desired property Standard deviation Standard deviation of the desired property Minimum Smallest value observed of the desired property Maximum Largest value oserved of the desired property Count Number of times a non-zero value is observed of the desired property Without losing generality, imagine we start out with this 4x4 matrix of observations and we want to plot 4 pixels (2 x 2). C1 C2 C3 C4 R1 1 2 3 4 R2 5 6 7 8 R3 9 10 11 12 R4 13 14 15 16 Aggregation by summation C1-2 C3-4 R1-2 14 22 R3-4 46 54 Aggregation by mean C1-2 C3-4 R1-2 3.5 5.5 R3-4 11.5 13.5 Aggregation by min C1-2 C3-4 R1-2 1 3 R3-4 9 11 Aggregation by max C1-2 C3-4 R1-2 6 8 R3-4 14 16 Aggregation by count C1-2 C3-4 R1-2 4 4 R3-4 4 4","title":"Aggregation"},{"location":"aggregation/#aggregating-datasets","text":"","title":"Aggregating datasets"},{"location":"aggregation/#motivation","text":"Tomahawk generally output many millions to many hundreds of millions, or even billions, of output linkage disequilibrium (LD) associations generated from many millions of input SNVs. It is technically very challenging to visualize such large datasets. Not only because of hardware limitations such as loading all the data into memory, or directly rendering billions of data points, but also because of more practical considerations such as cramming such a vast number of data points into a finite number of pixels would result in an absolute horrendous and uninformative image. In order to get a scope of the scale this problem presents, take for example a small chromosome like chr20 with data from the 1000 Genomes Project Phase 3 (1KGP3). This data comprises of 1,733,484 diploid SNVs. Assuming we can plot the LD data for a pair of SNVs in a single pixel, a monitor would have to have the dimensions 400 x 400 meters to display this data*! Not only would the monitor have to be huge, the memory requirement for plotting this image would be around 400 GB! Here we describe methods to overcome these obstacles. * Assuming a 1920 x 1080 pixel resolution and 20\" monitor as reference","title":"Motivation"},{"location":"aggregation/#existing-solutions","text":"There are several existing solutions for aggregating large datasets, such as Datashader for Python users. But packages like this requires us to leave the highly compressed internal binary representation of Tomahawk in order to transform two records into a form understandable by these frameworks. We have tried several of the most popular framework for aggregating datasets and found none that works in reasonable memory and is sufficiently efficient when applied to our specific use-case.","title":"Existing solutions"},{"location":"aggregation/#aggregation","text":"Aggregation is the process of reducing larger datasets to smaller ones for the purposes of displaying more data than can fit on the screen at once while maintaining the primary features of the original dataset. Tomahawk performs aggregation into regular grids (two-dimensional partitions) by applying summary statistics function on data collected in the given bins. At the moment, Tomahawk supports aggregation by Function Action Summation Sum total of the desired property Summation squared Sum total of squares of the desired property Mean Mean of the desired property Standard deviation Standard deviation of the desired property Minimum Smallest value observed of the desired property Maximum Largest value oserved of the desired property Count Number of times a non-zero value is observed of the desired property Without losing generality, imagine we start out with this 4x4 matrix of observations and we want to plot 4 pixels (2 x 2). C1 C2 C3 C4 R1 1 2 3 4 R2 5 6 7 8 R3 9 10 11 12 R4 13 14 15 16 Aggregation by summation C1-2 C3-4 R1-2 14 22 R3-4 46 54 Aggregation by mean C1-2 C3-4 R1-2 3.5 5.5 R3-4 11.5 13.5 Aggregation by min C1-2 C3-4 R1-2 1 3 R3-4 9 11 Aggregation by max C1-2 C3-4 R1-2 6 8 R3-4 14 16 Aggregation by count C1-2 C3-4 R1-2 4 4 R3-4 4 4","title":"Aggregation"},{"location":"algorithm-overview/","text":"Algorithmic overview \u00b6 Vectors of genotypes are represented as fixed-width run-length encoded (RLE) objects. This encoding scheme is generally superior to dynamic-width encoding approaches in terms of iteration speed (as no data processing is required) but inferior in terms of compressibility (as bits are wasted). The word-width of the RLE entries is fixed across a file and is determined contextually given the total number of samples. We describe three efficient algorithms to calculate genome-wide linkage disequilibrium for all pairwise alleles/genotypes in large-scale cohorts. The algorithms exploit different concepts: 1) low genetic diversity and 2) large memory registers on modern processors. The first algorithm directly compares fixed-width compressed RLE entries from two vectors in worst-case O(|RLE_A| + |RLE_B| + 1)-time. The second transforms compressed RLE entries to uncompressed k-bit-vectors and use machine-optimized SIMD-instructions to horizontally compare two such bit-vectors in worst-case O(N/W)-time. This algorithm also exploits the relatively low genetic diversity within species using implicit heuristics. The third algorithm computes summary statistics only by maintaining a positional index of non-reference alleles and associated uncompressed 1-bit-vectors for each genotypic vector in guaranteed O(min(|NON_REF_A|,|NON_REF_B))-time. The 1-bit vectors in this algorithm is different compared to the ones used in algorithm 2.","title":"Algorithm overview"},{"location":"algorithm-overview/#algorithmic-overview","text":"Vectors of genotypes are represented as fixed-width run-length encoded (RLE) objects. This encoding scheme is generally superior to dynamic-width encoding approaches in terms of iteration speed (as no data processing is required) but inferior in terms of compressibility (as bits are wasted). The word-width of the RLE entries is fixed across a file and is determined contextually given the total number of samples. We describe three efficient algorithms to calculate genome-wide linkage disequilibrium for all pairwise alleles/genotypes in large-scale cohorts. The algorithms exploit different concepts: 1) low genetic diversity and 2) large memory registers on modern processors. The first algorithm directly compares fixed-width compressed RLE entries from two vectors in worst-case O(|RLE_A| + |RLE_B| + 1)-time. The second transforms compressed RLE entries to uncompressed k-bit-vectors and use machine-optimized SIMD-instructions to horizontally compare two such bit-vectors in worst-case O(N/W)-time. This algorithm also exploits the relatively low genetic diversity within species using implicit heuristics. The third algorithm computes summary statistics only by maintaining a positional index of non-reference alleles and associated uncompressed 1-bit-vectors for each genotypic vector in guaranteed O(min(|NON_REF_A|,|NON_REF_B))-time. The 1-bit vectors in this algorithm is different compared to the ones used in algorithm 2.","title":"Algorithmic overview"},{"location":"api-documentation/","text":"C++ API documentation \u00b6 Under construction. Coming soon.","title":"API documentation"},{"location":"api-documentation/#c-api-documentation","text":"Under construction. Coming soon.","title":"C++ API documentation"},{"location":"developer-documentation/","text":"Developer documentation \u00b6 Under construction. Coming soon.","title":"Developer documentation"},{"location":"developer-documentation/#developer-documentation","text":"Under construction. Coming soon.","title":"Developer documentation"},{"location":"docker-tutorial/","text":"Getting started with Docker \u00b6 About \u00b6 Docker uses operating-system-level virtualization into \"containers\". With Docker we can create virtual environments that isolate a Tomahawk installation from the rest of the system. Tomahawk programs are run within this virtual environment that can share resources with its host machine. The Tomahawk Docker images are tested for each release. Prerequisites \u00b6 To follow this tutorial, you will need the following: One Ubuntu 18.04 server set up by following the Ubuntu 18.04 initial server setup guide, including a sudo non-root user and a firewall. An account on Docker Hub if you wish to create your own images and push them to Docker Hub, as shown in Steps 7 and 8. Getting started \u00b6 You can pull the latest Tomahawk image using 1 docker pull mklarqvist/tomahawk You can then run Tomahawk directly from that image. 1 docker run -v /var/data/:/data latest mklarqvist/tomahawk:latest tomahawk calc -pi input.twk -o output.two Tutorial complete Note: You now know how to use containerized Tomahawk. You can run detach the docker instance so that it runs in the background by passing the -d flag. 1 docker run -d -v /var/data/:/data latest mklarqvist/tomahawk:latest tomahawk calc -pi input.twk -o output.two Let's demonstrate some more Tomahawk Docker recipes. You can start a bash shell session within a Tomahawk container: 1 docker run -it -v /var/data/:/data latest mklarqvist/tomahawk:latest Within the container you can interactively run the tomahawk CLI or develop using the C++ API. If you want to stop and cancel a running docker instance then run docker ps to retrieve the target container id (first column) and execute docker stop CONTAINER_ID . Tutorial complete Success: You now know how to use containerized Tomahawk.","title":"Docker"},{"location":"docker-tutorial/#getting-started-with-docker","text":"","title":"Getting started with Docker"},{"location":"docker-tutorial/#about","text":"Docker uses operating-system-level virtualization into \"containers\". With Docker we can create virtual environments that isolate a Tomahawk installation from the rest of the system. Tomahawk programs are run within this virtual environment that can share resources with its host machine. The Tomahawk Docker images are tested for each release.","title":"About"},{"location":"docker-tutorial/#prerequisites","text":"To follow this tutorial, you will need the following: One Ubuntu 18.04 server set up by following the Ubuntu 18.04 initial server setup guide, including a sudo non-root user and a firewall. An account on Docker Hub if you wish to create your own images and push them to Docker Hub, as shown in Steps 7 and 8.","title":"Prerequisites"},{"location":"docker-tutorial/#getting-started","text":"You can pull the latest Tomahawk image using 1 docker pull mklarqvist/tomahawk You can then run Tomahawk directly from that image. 1 docker run -v /var/data/:/data latest mklarqvist/tomahawk:latest tomahawk calc -pi input.twk -o output.two Tutorial complete Note: You now know how to use containerized Tomahawk. You can run detach the docker instance so that it runs in the background by passing the -d flag. 1 docker run -d -v /var/data/:/data latest mklarqvist/tomahawk:latest tomahawk calc -pi input.twk -o output.two Let's demonstrate some more Tomahawk Docker recipes. You can start a bash shell session within a Tomahawk container: 1 docker run -it -v /var/data/:/data latest mklarqvist/tomahawk:latest Within the container you can interactively run the tomahawk CLI or develop using the C++ API. If you want to stop and cancel a running docker instance then run docker ps to retrieve the target container id (first column) and execute docker stop CONTAINER_ID . Tutorial complete Success: You now know how to use containerized Tomahawk.","title":"Getting started"},{"location":"installation/","text":"Installing Tomahawk \u00b6 Pre-requisites \u00b6 Unix-based operating system. Super-user priviledges if installing system-wide. Requirement Compiling and running Tomahawk requires a CPU with at least SSE4.2 available. This is a computational requirement stemming from the use of machine-optimized instructions called Single instruction, multiple data ( SIMD ). This processing paradigm enables us to perform operations on multiple datapoints simultaneously. The ( Streaming SIMD Extensions ) SSE4.2 instruction set, and later, describes the instructions for performing these operations and are embedded in the CPU. Installation instructions \u00b6 For modern x86-64 CPUs with SSE4.2 or later, just type make . If you see compilation errors, you most likely do not have SSE4.2 . At the present time, we do not support non-x86-64 CPUs or old CPU architecture. 1 2 3 git clone --recursive https://github.com/mklarqvist/tomahawk cd tomahawk make Debug mode \u00b6 If you are extending upon Tomahawk or debugging, we provide a DEBUG flag to build with all warnings triggered and with debug symbols enabled. See the main makefile for more information. 1 make DEBUG=true","title":"Installation"},{"location":"installation/#installing-tomahawk","text":"","title":"Installing Tomahawk"},{"location":"installation/#pre-requisites","text":"Unix-based operating system. Super-user priviledges if installing system-wide. Requirement Compiling and running Tomahawk requires a CPU with at least SSE4.2 available. This is a computational requirement stemming from the use of machine-optimized instructions called Single instruction, multiple data ( SIMD ). This processing paradigm enables us to perform operations on multiple datapoints simultaneously. The ( Streaming SIMD Extensions ) SSE4.2 instruction set, and later, describes the instructions for performing these operations and are embedded in the CPU.","title":"Pre-requisites"},{"location":"installation/#installation-instructions","text":"For modern x86-64 CPUs with SSE4.2 or later, just type make . If you see compilation errors, you most likely do not have SSE4.2 . At the present time, we do not support non-x86-64 CPUs or old CPU architecture. 1 2 3 git clone --recursive https://github.com/mklarqvist/tomahawk cd tomahawk make","title":"Installation instructions"},{"location":"installation/#debug-mode","text":"If you are extending upon Tomahawk or debugging, we provide a DEBUG flag to build with all warnings triggered and with debug symbols enabled. See the main makefile for more information. 1 make DEBUG=true","title":"Debug mode"},{"location":"introduction/","text":"Introduction \u00b6 The primary goal of tomahawk is to efficiently and conveniently compute linkage-disequilibrium for a single site vs its local neighbourhood, a sliding window across an interval, or all-vs-all pairwise genome-wide. Tomahawk is a C++ library with a standard CLI divided into various subroutines, as is the standard for most bioinformatics tools. tomahawk is much more efficient than existing solutions, both in terms of memory usage and compute time. tomahawk can easily compute genome-wide LD for cohorts of millions of samples over chromosome-scaled regions. tomahawk is designed as a C++ API to simplify LD-based workflow: either directly by using the C++ API or using any of the available language bindings. Currently there are R bindings and Python3 bindings . tomahawk has its own human-readable ( .ld ) interchange format and a highly compressed binary ( .two ) format for expedient analysis of the generated output data. Other text-based systems are extremely inefficient and do not support basic operations such as subsetting, searching, summarizing, and visualizing. tomahawk accepts any valid htslib -compatible input variant call format file for import into the internal binary tomahawk ( .twk ) file format.","title":"Introduction"},{"location":"introduction/#introduction","text":"The primary goal of tomahawk is to efficiently and conveniently compute linkage-disequilibrium for a single site vs its local neighbourhood, a sliding window across an interval, or all-vs-all pairwise genome-wide. Tomahawk is a C++ library with a standard CLI divided into various subroutines, as is the standard for most bioinformatics tools. tomahawk is much more efficient than existing solutions, both in terms of memory usage and compute time. tomahawk can easily compute genome-wide LD for cohorts of millions of samples over chromosome-scaled regions. tomahawk is designed as a C++ API to simplify LD-based workflow: either directly by using the C++ API or using any of the available language bindings. Currently there are R bindings and Python3 bindings . tomahawk has its own human-readable ( .ld ) interchange format and a highly compressed binary ( .two ) format for expedient analysis of the generated output data. Other text-based systems are extremely inefficient and do not support basic operations such as subsetting, searching, summarizing, and visualizing. tomahawk accepts any valid htslib -compatible input variant call format file for import into the internal binary tomahawk ( .twk ) file format.","title":"Introduction"},{"location":"job-balancing/","text":"Balancing compute \u00b6 Motivation \u00b6 Tomahawk stores variants in non-overlapping blocks. These blocks has to be compared either to themselves (diagonal) or against each other (square) in order to compute linkage disequilibrium (LD) for a set of variants. First we describe the rationale for pre-loading data into memory and then how to partition this subset in the most efficient way. Pre-loading data \u00b6 Without losing generality, consider the situation where you have a set of three blocks {1, 2, 3} that you want to compare pairwise. We describe the steps of a simple iterative algorithm to compare them below: Load blocks 1 and 2 into memory and compare {1, 2} Release block 2 from memory Load block 3 and compare {1, 3} Release block 1 from memory * Load block 2 from memory and compare {2, 3} Notice that we have now released and loaded the data for block 2 twice. This undesired memory- and IO-overhead grows square to the number of blocks, O((N-1) * (N-1) ) where N is the number of blocks. If you have 10 blocks, there will be 9 overhead loads for each block for a total of 81 excess loads. If you have 100 blocks, there will be 99 overhead loads for each block for a total of 9,801 excess loads * If you have 1000 blocks, there will be 999 overhead loads for each block for a total of 998,001 excess loads For example, using standard import parameters, chromosome 20 for the 1000 Genomes Project data has 1,696 blocks. Without addressing this problem, we would have an excess of 2,873,025 overhead loads. Because of this exorbant cost it is very desirable to load all the data of interest into memory just once. However, if done without careful effort to partition data of interest into subproblems then memory will become limiting very quickly. We describe how we address this problem in Tomahawk below. As an additional footnote, it is worthwile to mention that pre-loading data will spare the file-system on computer farms. This is generally always the most rate-limiting resource available. Memory-sparing job-loading \u00b6 Tomahawk splits large problems into multiple psuedo-balanced sub-problems in a memory-aware fashion using a tiling approach. Without losing generality, consider the situation where you want to calculate linkage disequilibrium (LD) for all variants pairwise. As a consequence, any given locus v will be compared to every other loci, V . The simplest, na\u00efve, way to parallelize this involves giving subproblem j out of J a list of loci from [i*j, (i+1)*j] for all subproblems, where i = V/J . This approach would invariantly require all data in memory irrespective of the slice-size. We can examplify this by drawing a square of four loci and divide the problem into three: {(1), (2), (3,4)}. Highlighted in bold are the sites addressed in subproblem 1 out of 3. Note that the lower triangular is not actually computed in practice and only shown here for visual clarity. Loci 1 2 3 4 1 1,1 2,1 3,1 4,1 2 1,2 2,2 3,2 4,2 3 1,3 2,3 3,3 4,3 4 1,4 2,4 3,4 4,4 In this approach, we need to have the data for variants {1, 2, 3, 4} when computing pairwise LD for locus {1}. In addition to the raw RLE objects, we would potentially have to allocate additional memory for the bit-vectors and masks. For most large datasets, this encompasses many tens of gigabytes. Not only does this require a large amount of memory but will also result in slower compute time because of the poor spatial locality of the data in memory. In order to overcome this restrictive boundary, we describe a slightly more complex load-balancing solution that revolves around the partitioning of the V^2 problem space into a square grid of subproblems. This will always be possible in the complete case as both dimensions are equal ( V and V ). In this case, each subproblem gets a list of loci from [i*j, (i+1)*j] in the imaginary x-dimension and [k*j, (k+1)*j] in the y-dimension. Slicing the data in two dimensions enables us to reduce memory usage down from O(V) to O(|k| + |l|) in the worst case. We can examplify this approach using the same example from above 1 2 3 4 1 1,1 2,1 3,1 4,1 2 1,2 2,2 3,2 4,2 3 1,3 2,3 3,3 4,3 4 1,4 2,4 3,4 4,4 In this approach, we only need to have data for {1,2} when computing LD for {1,2}. This memory-sparing approach dramatically reduces the memory-requirements per job on most datasets. For this approach to generally work out, we need to choose a slice-size such that its upper triangular plus diagonal equals the number of subproblems we want to solve. Here's the sequence of the first 50 valid slice-sizes: 1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 66, 78, 91, 105, 120, 136, 153, 171, 190, 210, 231, 253, 276, 300, 325, 351, 378, 406, 435, 465, 496, 528, 561, 595, 630, 666, 703, 741, 780, 820, 861, 903, 946, 990, 1035, 1081, 1128, 1176, 1225, 1275 You can generate this list in R : 1 choose ( 1 : 50 , 2 ) + 1 : 50 # triangular + diagonal and make note that this is of course equivalent to 1 ( ( 1 : 50 ) ^2 + ( 1 : 50 ) ) / 2 # (square + diagonal) / 2 The total number of subproblems to solve ( -c ) has to be a member of this function. Practical difference \u00b6 We can demonstrate the efficiency of our grid-partitioning method even on small chromosomes like chr20 using data from the 1000 Genomes Project Phase 3 (1KGP3). This data comprises of 2,504 samples with 1,733,484 diploid SNVs. As expected, when partitioning the problem as described we see a propotional decrease in memory allocation with the number of subproblems. Approach Memory Na\u00efve 5 GB Tiling-10 1312 MB Tiling-45 608 MB Tiling-105 398 MB","title":"Job balancing"},{"location":"job-balancing/#balancing-compute","text":"","title":"Balancing compute"},{"location":"job-balancing/#motivation","text":"Tomahawk stores variants in non-overlapping blocks. These blocks has to be compared either to themselves (diagonal) or against each other (square) in order to compute linkage disequilibrium (LD) for a set of variants. First we describe the rationale for pre-loading data into memory and then how to partition this subset in the most efficient way.","title":"Motivation"},{"location":"job-balancing/#pre-loading-data","text":"Without losing generality, consider the situation where you have a set of three blocks {1, 2, 3} that you want to compare pairwise. We describe the steps of a simple iterative algorithm to compare them below: Load blocks 1 and 2 into memory and compare {1, 2} Release block 2 from memory Load block 3 and compare {1, 3} Release block 1 from memory * Load block 2 from memory and compare {2, 3} Notice that we have now released and loaded the data for block 2 twice. This undesired memory- and IO-overhead grows square to the number of blocks, O((N-1) * (N-1) ) where N is the number of blocks. If you have 10 blocks, there will be 9 overhead loads for each block for a total of 81 excess loads. If you have 100 blocks, there will be 99 overhead loads for each block for a total of 9,801 excess loads * If you have 1000 blocks, there will be 999 overhead loads for each block for a total of 998,001 excess loads For example, using standard import parameters, chromosome 20 for the 1000 Genomes Project data has 1,696 blocks. Without addressing this problem, we would have an excess of 2,873,025 overhead loads. Because of this exorbant cost it is very desirable to load all the data of interest into memory just once. However, if done without careful effort to partition data of interest into subproblems then memory will become limiting very quickly. We describe how we address this problem in Tomahawk below. As an additional footnote, it is worthwile to mention that pre-loading data will spare the file-system on computer farms. This is generally always the most rate-limiting resource available.","title":"Pre-loading data"},{"location":"job-balancing/#memory-sparing-job-loading","text":"Tomahawk splits large problems into multiple psuedo-balanced sub-problems in a memory-aware fashion using a tiling approach. Without losing generality, consider the situation where you want to calculate linkage disequilibrium (LD) for all variants pairwise. As a consequence, any given locus v will be compared to every other loci, V . The simplest, na\u00efve, way to parallelize this involves giving subproblem j out of J a list of loci from [i*j, (i+1)*j] for all subproblems, where i = V/J . This approach would invariantly require all data in memory irrespective of the slice-size. We can examplify this by drawing a square of four loci and divide the problem into three: {(1), (2), (3,4)}. Highlighted in bold are the sites addressed in subproblem 1 out of 3. Note that the lower triangular is not actually computed in practice and only shown here for visual clarity. Loci 1 2 3 4 1 1,1 2,1 3,1 4,1 2 1,2 2,2 3,2 4,2 3 1,3 2,3 3,3 4,3 4 1,4 2,4 3,4 4,4 In this approach, we need to have the data for variants {1, 2, 3, 4} when computing pairwise LD for locus {1}. In addition to the raw RLE objects, we would potentially have to allocate additional memory for the bit-vectors and masks. For most large datasets, this encompasses many tens of gigabytes. Not only does this require a large amount of memory but will also result in slower compute time because of the poor spatial locality of the data in memory. In order to overcome this restrictive boundary, we describe a slightly more complex load-balancing solution that revolves around the partitioning of the V^2 problem space into a square grid of subproblems. This will always be possible in the complete case as both dimensions are equal ( V and V ). In this case, each subproblem gets a list of loci from [i*j, (i+1)*j] in the imaginary x-dimension and [k*j, (k+1)*j] in the y-dimension. Slicing the data in two dimensions enables us to reduce memory usage down from O(V) to O(|k| + |l|) in the worst case. We can examplify this approach using the same example from above 1 2 3 4 1 1,1 2,1 3,1 4,1 2 1,2 2,2 3,2 4,2 3 1,3 2,3 3,3 4,3 4 1,4 2,4 3,4 4,4 In this approach, we only need to have data for {1,2} when computing LD for {1,2}. This memory-sparing approach dramatically reduces the memory-requirements per job on most datasets. For this approach to generally work out, we need to choose a slice-size such that its upper triangular plus diagonal equals the number of subproblems we want to solve. Here's the sequence of the first 50 valid slice-sizes: 1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 66, 78, 91, 105, 120, 136, 153, 171, 190, 210, 231, 253, 276, 300, 325, 351, 378, 406, 435, 465, 496, 528, 561, 595, 630, 666, 703, 741, 780, 820, 861, 903, 946, 990, 1035, 1081, 1128, 1176, 1225, 1275 You can generate this list in R : 1 choose ( 1 : 50 , 2 ) + 1 : 50 # triangular + diagonal and make note that this is of course equivalent to 1 ( ( 1 : 50 ) ^2 + ( 1 : 50 ) ) / 2 # (square + diagonal) / 2 The total number of subproblems to solve ( -c ) has to be a member of this function.","title":"Memory-sparing job-loading"},{"location":"job-balancing/#practical-difference","text":"We can demonstrate the efficiency of our grid-partitioning method even on small chromosomes like chr20 using data from the 1000 Genomes Project Phase 3 (1KGP3). This data comprises of 2,504 samples with 1,733,484 diploid SNVs. As expected, when partitioning the problem as described we see a propotional decrease in memory allocation with the number of subproblems. Approach Memory Na\u00efve 5 GB Tiling-10 1312 MB Tiling-45 608 MB Tiling-105 398 MB","title":"Practical difference"},{"location":"r-tutorial/","text":"Getting started with rtomahawk \u00b6 About \u00b6 This is an introductory tutorial for using the R-bindings for Tomahawk ( rtomahawk ). It will cover: Importing data into Tomahawk Loading, subsetting, and filtering output data Computing linkage-disequilibrium Plotting raw linkage-disequilibrium data Aggregating and visualizing datasets Combining LD information with GWAS P-values and annotations Prerequisites \u00b6 In order to follow this tutorial exactly, you will need to download the variant call data for chromosome 6 the 1000 Genomes Project (1KGP3) cohort and validate its correctness. You can use the following commands on Debian-based systems, or, go to the 1KGP3 FTP server and download the data manually. 1 2 wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf. { gz,gz.tbi } md5sum ALL.chr6.*.gz* MD5 checksum File 9be06b094cc80281c9aec651d657abb9 ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz 5ed21968b2e212fa2cc9237170866c5b ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz.tbi This tutorial was designed and tested for 1 2 3 > tomahawkVersion () rtomahawk : 0.1.0 Libraries : tomahawk -0.7.0 ; ZSTD -1.3.1 ; htslib 1.9 Import files into Tomahawk \u00b6 Depending on the downstream application you want to import either Tomahawk representations of sequence variant files ( .twk ) or Tomahawk-generated output LD data ( .two ). Both operations require minimal work. First we will import a vcf / vcf.gz / bcf file into the binary Tomahawk file format ( .twk ): 1 twk <- import ( \"ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\" , \"1kgp3_chr6\" ) Auto-completion of file extensions You do not have to append the .twk suffix to the output name as Tomahawk will automatically add this if missing. In the example above \"1kgp3_chr6\" will be converted to \"1kgp3_chr6.two\" automatically. This is true for most Tomahawk commands when using the CLI but not when using rtomahawk . Unsafe method In the current release of rtomahawk , this subroutine does not check for user-interruption (for example Ctrl+C or Ctrl+Z ) commands. This means that you need to wait until the underlying process has finished or you have to terminate the host R session, in turn killing the spawned process. This will be fixed in upcoming releases. By default, rtomahawk will print verbose output to the console during the importing procedure. This will take several minutes depending on your system. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [2019-01-21 10:42:59,178][LOG][READER] Opening ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz... [2019-01-21 10:42:59,188][LOG][VCF] Constructing lookup table for 86 contigs... [2019-01-21 10:42:59,188][LOG][VCF] Samples: 2,504... [2019-01-21 10:42:59,188][LOG][WRITER] Opening 1kgp3_chr6.twk... [2019-01-21 10:44:41,225][LOG] Duplicate site dropped: 6:18233985 [2019-01-21 10:48:06,492][LOG] Duplicate site dropped: 6:55137646 [2019-01-21 10:49:01,472][LOG] Duplicate site dropped: 6:67839893 [2019-01-21 10:49:36,487][LOG] Duplicate site dropped: 6:74373442 [2019-01-21 10:49:55,386][LOG] Duplicate site dropped: 6:77843171 [2019-01-21 10:53:47,992][LOG] Duplicate site dropped: 6:121316830 [2019-01-21 10:56:08,853][LOG] Duplicate site dropped: 6:148573620 [2019-01-21 10:56:25,301][LOG] Duplicate site dropped: 6:151397786 [2019-01-21 10:58:16,584][LOG] Wrote: 4,784,608 variants to 9,570 blocks... [2019-01-21 10:58:16,584][LOG] Finished: 15m17,405s [2019-01-21 10:58:16,584][LOG] Filtered out 239,511 sites (4.76722%): [2019-01-21 10:58:16,584][LOG] Invariant: 15,485 (0.308213%) [2019-01-21 10:58:16,584][LOG] Missing threshold: 0 (0%) [2019-01-21 10:58:16,584][LOG] Insufficient samples: 0 (0%) [2019-01-21 10:58:16,584][LOG] Mixed ploidy: 0 (0%) [2019-01-21 10:58:16,584][LOG] No genotypes: 0 (0%) [2019-01-21 10:58:16,584][LOG] No FORMAT: 0 (0%) [2019-01-21 10:58:16,584][LOG] Not biallelic: 26,277 (0.523017%) [2019-01-21 10:58:16,584][LOG] Not SNP: 194,566 (3.87264%) The import procedure will return a new empty twk class with a file pointer set to the newly created output file: 1 2 > twk An object of class twk Importing variant call archives Success: This object is now ready to be used by other downstream functions in rtomahawk . Loading and reading .two data \u00b6 Many functions in rtomahawk computes or use linkage-disequilibrium and require a different loading procedure involving the openTomahawkOuput subroutine. In this example we have a pre-computed .two output file available locally. We open this file and load all supportive information such as headers, footers, sample information, and validate the archive and collate this information in a new twk class together with a file pointer to the target archive: 1 twk <- openTomahawkOutput ( \"example.two\" ) Auto-completion of file extensions This function will not search for files in the directly by auto-completing missing file extensions. This is done purposely to avoid enforcing file extensions and any possible capitalization issues. This procedure is extremely fast as very little data is loaded from the archive and instead maintaining a data pointer. This approach, albeit a bit akward, enables us to handle more data than there is available system memory (RAM). 1 2 3 > system.time ( twk <- openTomahawkOutput ( \"example.two\" )) user system elapsed 0.003 0.000 0.002 Opening pre-computed LD data Success: This object is now ready to be used by other downstream functions in rtomahawk . In some situations you do want to load data into memory using the file handle pointer in a twk object. This is trivially done with the readRecords subroutine that require a twk class object as a required parameter. As it is very simple to accidentally load hundreds of millions of records if you are not careful we introduced the logical flag really . By default this flag is set to FALSE and will force terminate the loading procedure at 10 million records. 1 y <- readRecords ( twk , really = FALSE ) If the safety limit is reached a warning message will be printed to the console: 1 2 > y <- readRecords ( twk , really = FALSE ) limit reached Source of memory inefficiency In the current release, this subroutine load records (C structs) into vectors of records. This is unlike the required column-store format used in R . Because of this, we perform an implicit transposition of the internal structs into appropriate std::vectors followed by convertion into the appropriate data.table format. During this final convertion into SEXP structures, Rcpp will perform an unneccessary copy of the data resulting in a transient use (spike) of excess memory. We will address this problem in upcoming releases. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 > y An object of class twk Has internal data: 10000000 records FLAG ridA posA ridB posB REFREF REFALT ALTREF ALTALT D 1 11 6 123695 6 227960 4978 23 0 7 0.001389390 2 3 6 143500 6 228257 4425 387 78 118 0.019615739 3 3 6 144397 6 224822 4856 16 98 38 0.007295037 4 3 6 144398 6 224822 4856 16 98 38 0.007295037 5 3 6 150279 6 225420 4229 345 233 201 0.030687481 Dprime R R2 P ChiSqFisher ChiSqModel 1 1.0000000 0.4819338 0.2322602 1.304179e-16 1163.1592 0 2 0.5574107 0.3359277 0.1128474 5.789946e-71 565.1399 0 3 0.6954327 0.4345684 0.1888497 1.786734e-49 945.7594 0 4 0.6954327 0.4345684 0.1888497 1.786734e-49 945.7594 0 5 0.3974391 0.3499740 0.1224818 8.508703e-90 613.3890 0 ... FLAG ridA posA ridB posB REFREF REFALT ALTREF ALTALT D 9999995 3 6 1834075 6 1888336 4932 4 52 20 0.003924711 9999996 11 6 1834098 6 1885227 4892 0 97 19 0.003706051 9999997 11 6 1834098 6 1886103 4982 0 7 19 0.003774233 9999998 11 6 1834098 6 1890443 4873 0 116 19 0.003691657 9999999 11 6 1834098 6 1890998 4873 0 116 19 0.003691657 10000000 11 6 1834098 6 1891303 4873 0 116 19 0.003691657 Dprime R R2 P ChiSqFisher ChiSqModel 9999995 0.8309022 0.4774060 0.2279165 8.176940e-35 1141.4056 0 9999996 1.0000000 0.4007599 0.1606085 1.856439e-32 804.3274 0 9999997 1.0000000 0.8542505 0.7297439 4.211277e-48 3654.5574 0 9999998 1.0000000 0.3707673 0.1374684 4.173075e-31 688.4415 0 9999999 1.0000000 0.3707673 0.1374684 4.173075e-31 688.4415 0 10000000 1.0000000 0.3707673 0.1374684 4.173075e-31 688.4415 0 A more useful procedure is to slice out regions of interest that match some set of criteron. Such as for example retrieving records from the region chr6:5e6-10e6 with an R2 value between 0.4 and 0.8 and a P-value < 0.001: 1 2 f <- setFilters ( minR2 = 0.6 , maxR2 = 0.8 , maxP = 0.001 ) recs <- readRecords ( twk , \"6:5e6-10e6\" , filters = f , really = TRUE ) Interval slicing Intervals are left-inclusive and right-exclusive [A, B) and use 1-base coordinates. The interval syntax has to match either: A) contig , B) contig:pos , or C) contig:pos-pos . It is important to note that interval slicing is performed on the forward position only ( posA ) and that by default the output data generarted from Tomahawk have bidirectional symmetry such that two tuples (A,B) == (B,A), exists and are mirrored. We can ascertain that the filtering procedure work correctly by looking at the summary statistics for each column and note that posA is bounded by 5Mb-10Mb, R2 is bounded by [0.6, 0.8], and P < 0.001: 1 summary ( recs ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 Min. 1st Qu. Median Mean 3rd Qu. FLAG 3.000000e+00 3.000000e+00 3.000000e+00 5.311864e+01 3.000000e+00 ridA 3.659840e+05 3.659840e+05 3.659840e+05 3.659840e+05 3.659840e+05 posA 5.000012e+06 7.093750e+06 7.887604e+06 7.780695e+06 8.673979e+06 ridB 3.659840e+05 3.659840e+05 3.659840e+05 3.659840e+05 3.659840e+05 posB 2.869352e+06 7.094321e+06 7.887945e+06 7.782022e+06 8.675012e+06 REFREF 0.000000e+00 2.801000e+03 4.728000e+03 3.836738e+03 4.953000e+03 REFALT 0.000000e+00 2.000000e+00 1.800000e+01 2.416587e+02 1.300000e+02 ALTREF 0.000000e+00 3.000000e+00 2.600000e+01 2.241404e+02 1.630000e+02 ALTALT 0.000000e+00 3.500000e+01 1.630000e+02 7.054628e+02 1.076000e+03 D -2.231411e-01 4.564901e-03 2.287482e-02 5.856900e-02 1.283542e-01 Dprime 7.749466e-01 8.886666e-01 9.506363e-01 9.368918e-01 9.961449e-01 R 7.745978e-01 8.045203e-01 8.354463e-01 8.354627e-01 8.654019e-01 R2 6.000017e-01 6.472530e-01 6.979705e-01 6.992205e-01 7.489204e-01 P 0.000000e+00 0.000000e+00 2.578244e-297 2.630748e-12 2.487910e-85 ChiSqFisher 3.004809e+03 3.241443e+03 3.495436e+03 3.501696e+03 3.750593e+03 ChiSqModel 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 Max. FLAG 2.063000e+03 ridA 3.659840e+05 posA 9.999976e+06 ridB 3.659840e+05 posB 1.240232e+07 REFREF 5.003000e+03 REFALT 4.945000e+03 ALTREF 5.005000e+03 ALTALT 4.999000e+03 D 2.233769e-01 Dprime 1.000000e+00 R 8.944252e-01 R2 7.999965e-01 P 2.392816e-07 ChiSqFisher 4.006383e+03 ChiSqModel 0.000000e+00 Opening and slicing LD data Success: This object is now ready to be used by other downstream functions in rtomahawk . There are a large number of filter parameters available to slice out the exact data of interest: Field Type flagInclude integer flagExclude integer minR numeric maxR numeric minR2 numeric maxR2 numeric minD numeric maxD numeric minDprime numeric maxDprime numeric minP numeric maxP numeric minP1 numeric maxP1 numeric minP2 numeric maxP2 numeric minQ1 numeric maxQ1 numeric minQ2 numeric maxQ2 numeric minChiSqFisher numeric maxChiSqFisher numeric minChiSqModel numeric maxChiSqModel numeric upperOnly logical lowerOnly logical See ?setFilters for more information. Calculating linkage-disequilibrium \u00b6 Plotting data with rtomahawk \u00b6 Color schemes \u00b6 rtomahawk comes prepacked with a number of color schemes. Most of these are taken directly from the viridis R package and are included here independent of that package to remove the package interdependency. These color schemes are callable as functions taking as require arguments the number of distinct values ( n ) and optionally the desired opacity (alpha) level ( alpha ). Color scheme viridis plasma magma inferno cividis default We can plot these values as a gradient of values from [0, 100) and as [0, 11] by calling the displayColors function: 1 displayColors () Square representation \u00b6 Graphically representing LD data is useful for checking data quality and in exploration. If your data has already been loaded into memory using readRecords it is possible to plot this data as individual data points using the plotLD function. This approach is generally only feasable for visualizing smaller genomic regions, for example, < 2-3 megabases. If you are investigating longer ranges than this you should consider using the aggregation functions provided in tomahawk / rtomahawk . Because of the vast number of data points rendered and the finite amount of pixels available, we render data points with an opacity gradient scaled according to its R2 value from [0.1, 1]. This allows for mixing of both colors and opacities to more clearly represent the distribution of the underlying data. It is possible to disable this functionality by setting the optional argument opacity to FALSE . In the following examples, we render both a large region (5-8 Mb) and a small region (5.0-5.6 Mb) with and without the opacity flag set. We also showcase the different color schemes. 1 2 3 4 5 6 7 8 9 10 11 # Load some local data into memory. twk <- openTomahawkOutput ( \"1kgp3_chr6.two\" ) y <- readRecords ( twk , \"6:5e6-10e6\" , really = TRUE ) # Plot two panels in the same figure. # Top panel: opacity gradient from [0.1, 1.0] mapping to # R2 range [0.1, 0.1, 0.2, ..., 1.0] # <COLOR> placeholder gets replaced with one of the color # schemes described below. par ( mfrow = c ( 2 , 1 )) plotLD ( y , ylim = c ( 5e6 , 8e6 ), xlim = c ( 5e6 , 8e6 ), colors =< COLOR > , bg =< COLOR > ( 11 ) [1] ) plotLD ( y , ylim = c ( 5e6 , 8e6 ), xlim = c ( 5e6 , 8e6 ), colors =< COLOR > , bg =< COLOR > ( 11 ) [1] , opacity = FALSE ) Color scheme Image default cividis inferno magma plasma viridis By default, the birectionally symmetric output data from tomahawk is kept (i.e. both (A,B) and (B,A)) tuples are kept. For this reason, the output plot will be square (or rectangular with mismatched xlim and ylim parameters). In some cases, only the upper or lower triangular values are desired. We can control what values are plotted with the logical upper and lower parameters. 1 2 3 4 par ( mfrow = c ( 1 , 3 )) plotLD ( y , ylim = c ( 5e6 , 8e6 ), xlim = c ( 5e6 , 8e6 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] ) plotLD ( y , ylim = c ( 5e6 , 8e6 ), xlim = c ( 5e6 , 8e6 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , upper = T ) plotLD ( y , ylim = c ( 5e6 , 8e6 ), xlim = c ( 5e6 , 8e6 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , lower = T ) Triangular representation \u00b6 In many cases, there is generally no need to graphically represent the entire square (or rectangular) symmetric matrix of associations. This is especially true when combining multiple graphs together to create a more comprehensive picture of a particular region or feature. The topic of combining plots will be explored in greter detail below. All of the examples here involves the subroutine plotLDTriangular . As a design choice, we decided to restrict the rendered y-axis data such that it is always bounded by the x-axis limits. For this reason, these plots will always be triangular will partially \"missing\" (omitted) values even if they are technically present in the dataset. First, we describe its most basic use case. In the following example, we will plot a section of associations with the viridis color palette and specifiy the background to be the lowest (in this case, first) color in that scheme. 1 plotLDTriangular ( y , colors = viridis ( 11 ), bg = viridis ( 11 ) [1] ) Because of the smallish haplotype blocks in humans, most of these visible triangular structures will have limited span in the y-axis. We can truncate the y-axis to zoom into the local neighbourhood and more accurately display the local haplotype structure. 1 plotLDTriangular ( y , colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , ylim = c ( 0 , 300e3 )) It is possible to control the orientation (rotation) of the output graph by specifying the orientation parameter. The numerial encodings are: 1) standard; 2) upside down; 3) left-right flipped; and 4) right-left flipped. 1 2 3 4 5 par ( mfrow = c ( 2 , 2 )) plotLDTriangular ( y , ylim = c ( 0 , 300e3 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 1 ) plotLDTriangular ( y , ylim = c ( 0 , 300e3 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 2 ) plotLDTriangular ( y , ylim = c ( 0 , 300e3 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 3 ) plotLDTriangular ( y , ylim = c ( 0 , 300e3 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 4 ) It is possible to completely disable all anotation by setting the logical parameter annotate to FALSE . This will remove titles, axes, and ticks. 1 plotLDTriangular ( y , ylim = c ( 0 , 300e3 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 1 , annotate = FALSE ) Note that these plotting functions respect the global mar (margin) values and (by default) will have white space around it. We can change the global par argument to, for example, zero to remove these margins when annotation is disabled for edge-to-edge graphics. Visualizing GWAS data and LD \u00b6 In this section we will produce LocusZoom-like plots using GWAS data from the UK BioBank comprised exclusively of white British individuals. The Roslin Institute at the University of Edinbrugh host a data browser of associatins called Gene Atlas . In the following examples we will investigate the association of genotypes at chromosome 6 and diabetes in this cohort. Data in its entirety can be explored further using the Gene Atlas. To reproduce the results below download the imputed data for chromosome 6 and the associated positional information . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Downloaded data from: # http://static.geneatlas.roslin.ed.ac.uk/gwas/allWhites/imputed/data.copy/imputed.allWhites.selfReported_n_1245.chr6.csv.gz # http://static.geneatlas.roslin.ed.ac.uk/gwas/allWhites/snps/extended/snps.imputed.chr6.csv.gz library ( data.table ) # For speedier reading of data. # We zcat (uncompress gzipped archive) directly into fread. # This may not work for Windows users. In that case, decompress # the files manually first. x <- fread ( \"zcat imputed.allWhites.selfReported_n_1245.chr6.csv.gz\" , sep = \" \" ) snp <- fread ( \"zcat snps.imputed.chr6.csv.gz\" , sep = \" \" ) # Keep matching data available in both files. We neeed to maintain # parity between the two sets. snp <- snp [match ( x $ SNP , snp $ SNP ), ] # Transform linear-scale (untransformed) P-values into -log10(P). snp $ p <- - log10 ( x $ `PV-selfReported_n_1245` ) # Setup path to local Tomahawk file to compute LD against. twk2 <- new ( \"twk\" ) twk2 @ file.path <- \"1kgp3_chr6.twk\" # Load human recombination data (hg19) supplied with `rtomahawk` data ( gmap ) # Region of 1 Mb in either direction of target. single <- plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 1e6 , minR2 = 0 ) # Region of 50 Kb in either direction of target. single <- plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 50e3 , minR2 = 0 ) Internal LD computation The plotLZ plotting function will internally compute linkage-disequilibrium for a target SNV and it's surrounding genomic region. This background computation is stored in a temporary file and then loaded back into memory and returned as a new twk class instance. If you have further use of this information then you need to capture the return value of this function. It is possible to change the default temp directory used in R with the subroutine set.tempdir . Please note that all data written to this temporary directory will be purged upon exiting the current session of R . These functions are extremely fast as the R-bindings use .Call commands to communicate with the compiled C++ shared object: 1 2 3 4 5 6 7 > system.time ( plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 1e6 , minR2 = 0 )) user system elapsed 1.405 0.001 1.407 > system.time ( plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 50e3 , minR2 = 0 )) user system elapsed 0.115 0.004 0.119 Almost all of this time is spent rendering symbols in the vectorized plot. The same command using the CLI takes roughly 1/3rd of the time: 1 2 3 4 $ time tomahawk scalc -i 1kgp3_chr6.twk -I 6 :20694884 -w 1000000 > /dev/null real 0m0.452s user 0m0.937s sys 0m0.061s Many aspects of the plots can be customized to your needs. For example, modifying the datapoint symbol pch in the valid range [21, 25]. Note that all other pch values are technically valid but will not generate a fill color. If your visualizations require additional data layers it is possible to combine these into a single plot as rtomahawk renders plots using base-R. In this example we will combine two plots generated by rtomahawk : the GWAS P-value and its single-site LD together with the all-vs-all pairwise LD for the same region. 1 2 3 4 5 6 par ( mfrow = c ( 2 , 1 ), mar = c ( 0 , 5 , 3 , 5 )) # Set bottom margin to 0. single <- plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 1e6 , minR2 = 0 , xlab = \"\" , xaxt = \"n\" ) twk <- openTomahawkOutput ( \"test_region.two\" ) y <- readRecords ( twk , really = TRUE ) par ( mar = c ( 5 , 5 , 0 , 5 )) # Set top margin to 0. plotLDTriangular ( y , ylim = c ( 0 , 500e3 ), xlim = c ( 20694884 - 1e6 , 20694884 + 1e6 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 2 , cex = .25 , main = \"\" ) We can zoom into the local region (100kb flanking region) by simply changing the window parameter in plotLZ and the xlim range in plotLDTriangular . It is possible to add more advanced data layers using external packages with some simple manipulations. In this example we will add a gene track using genetic information extracted from biomaRt and drawn using Sushi , both third-party packages. biomaRt information The R package biomaRt requires internet connectivity to function as the package itself is a wrapper around the Ensembl BioMart tool that allows extraction of connected data from databases without having to perform explicit programming. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 library ( biomaRt ) mart <- useMart ( host = 'http://grch37.ensembl.org' , biomart = \"ensembl\" , dataset = \"hsapiens_gene_ensembl\" ) # We will retrieve results from biomaRt twice because the database # does _not_ allow simultaneous queries for exon-level data and # gene-level data. To overcome this problem, we will perform two # quries to the database and merge the results together. results <- getBM ( attributes = c ( \"ensembl_exon_id\" , \"exon_chrom_start\" , \"exon_chrom_end\" ), filters = c ( \"chromosome_name\" , \"start\" , \"end\" ), values = list ( 6 , 20694884-1e6 , 20694884+1e6 ), mart = mart ) results2 <- getBM ( attributes = c ( \"ensembl_exon_id\" , \"hgnc_symbol\" , \"chromosome_name\" , \"start_position\" , \"end_position\" , \"strand\" ), filters = c ( \"chromosome_name\" , \"start\" , \"end\" ), values = list ( 6 , 20694884-1e6 , 20694884+1e6 ), mart = mart ) results <- cbind ( results , results2[ , -1 , drop = F ] ) results <- results[results $ hgnc_symbol != \"\" , ] results <- results[ , c ( 2 , 3 , 4 , 5 , 8 ) ] names ( results ) <- c ( \"start\" , \"stop\" , \"gene\" , \"chrom\" , \"strand\" ) results $ score = \".\" results <- results[ , c ( \"chrom\" , \"start\" , \"stop\" , \"gene\" , \"score\" , \"strand\" ) ] library ( Sushi ) chrom = 6 chromstart = 20694884-1e6 chromend = 20694884+1e6 layout ( matrix ( 1 : 3 , ncol = 1 ), heights = c ( 2 , 2 , 1 )) par ( mar = c ( 0 , 5 , 3 , 5 )) # Set bottom margin to 0. single <- plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 1e6 , minR2 = 0 , xlab = \"\" , xaxt = \"n\" ) twk <- openTomahawkOutput ( \"test_region.two\" ) y <- readRecords ( twk , really = TRUE ) par ( mar = c ( 0 , 5 , 0 , 5 )) # Set top and bottom margins to 0. plotLDTriangular ( y , ylim = c ( 0 , 500e3 ), xlim = c ( 20694884 - 1e6 , 20694884 + 1e6 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 2 , cex = .25 , main = \"\" ) par ( mar = c ( 2 , 5 , 0 , 5 )) plotGenes ( geneinfo = results , chrom = chrom , chromstart = chromstart , chromend = chromend , labeloffset = .5 , fontsize = 1 , arrowlength = 0.025 ) abline ( v = 20694884 , lty = \"dashed\" , col = \"grey\" ) # Highjack internal `rtomahawk` function for drawing genomic axes. # This function is unexported and is not intended for general use. rtomahawk ::: addGenomicAxis ( c ( chromstart , chromend ), at = 1 , las = 1 , F ) Again, we can zoom into the local region (200kb flanking region) by simply changing the appropriate parameters in the code above. The plotting time can vary from milliseconds to several seconds depending on the number of points that are being computed directly (top panel) and the number of points that are loaded and rendered (middle panel). In the first example above, rtomahawk and tomahawk computes and plots millions of LD associations and data points in a few seconds on a single thread. 1 2 3 > system.time ( f ()) user system elapsed 6.336 0.260 6.233 In this code snippet, the function f() , is simply a placeholder for the code above. Aggregating and visualizing datasets \u00b6 Quantile-normalized (left) or linear range (right) Unsafe method In the current release of rtomahawk , this subroutine does not check for user-interruption (for example Ctrl+C or Ctrl+Z ) commands. This means that you need to wait until the underlying process has finished or you have to terminate the host R session, in turn killing the spawned process. This will be fixed in upcoming releases. 1 2 3 4 5 twk <- openTomahawkOutput ( \"example.two\" ) x <- aggregate ( twk , aggregation = \"r2\" , reduction = \"count\" , xbins = 1000 , ybins = 1000 , minCount = 50 , verbose = T , threads = 8 ) par ( mar = c ( 5 , 5 , 5 , 8 ), mfrow = c ( 1 , 2 )) plotAggregation ( x , normalize = TRUE ) plotAggregation ( x , normalize = FALSE ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 > agg <- aggregate ( twk , \"r2\" , \"count\" , 1000 , 1000 , 50 , verbose = T , threads = 6 ) Program : tomahawk -062954 eb ( Tools for computing , querying and storing LD data ) Libraries : tomahawk -0.7.0 ; ZSTD -1.3.1 ; htslib 1.9 Contact : Marcus D. R. Klarqvist < mk819 @ cam.ac.uk > Documentation : https :// github.com / mklarqvist / tomahawk License : MIT ---------- [2019 -01-19 11 : 05 : 37 , 072 ][LOG] Calling aggregate... [2019 -01-19 11 : 05 : 37 , 072 ][LOG] Performing 2 - pass over data... [2019 -01-19 11 : 05 : 37 , 072 ][LOG] ===== First pass ( peeking at landscape ) ===== [2019 -01-19 11 : 05 : 37 , 072 ][LOG] Blocks : 32 [2019 -01-19 11 : 05 : 37 , 072 ][LOG] Uncompressed size : 5.286261 Gb [2019 -01-19 11 : 05 : 37 , 072 ][LOG][THREAD] Data / thread : 881.043564 Mb [2019 -01-19 11 : 05 : 41 , 040 ][LOG] ===== Second pass ( building matrix ) ===== [2019 -01-19 11 : 05 : 41 , 040 ][LOG] Aggregating 49 , 870 , 388 records... [2019 -01-19 11 : 05 : 41 , 040 ][LOG][THREAD] Allocating : 240.000000 Mb for matrices... [2019 -01-19 11 : 05 : 44 , 797 ][LOG] Aggregated 49 , 870 , 388 records in 1 , 000 , 000 bins. [2019 -01-19 11 : 05 : 44 , 798 ][LOG] Finished. Load a pre-computed tomahawk aggregation object: 1 agg <- loadAggregate ( \"agg.twa\" )","title":"R"},{"location":"r-tutorial/#getting-started-with-rtomahawk","text":"","title":"Getting started with rtomahawk"},{"location":"r-tutorial/#about","text":"This is an introductory tutorial for using the R-bindings for Tomahawk ( rtomahawk ). It will cover: Importing data into Tomahawk Loading, subsetting, and filtering output data Computing linkage-disequilibrium Plotting raw linkage-disequilibrium data Aggregating and visualizing datasets Combining LD information with GWAS P-values and annotations","title":"About"},{"location":"r-tutorial/#prerequisites","text":"In order to follow this tutorial exactly, you will need to download the variant call data for chromosome 6 the 1000 Genomes Project (1KGP3) cohort and validate its correctness. You can use the following commands on Debian-based systems, or, go to the 1KGP3 FTP server and download the data manually. 1 2 wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf. { gz,gz.tbi } md5sum ALL.chr6.*.gz* MD5 checksum File 9be06b094cc80281c9aec651d657abb9 ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz 5ed21968b2e212fa2cc9237170866c5b ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz.tbi This tutorial was designed and tested for 1 2 3 > tomahawkVersion () rtomahawk : 0.1.0 Libraries : tomahawk -0.7.0 ; ZSTD -1.3.1 ; htslib 1.9","title":"Prerequisites"},{"location":"r-tutorial/#import-files-into-tomahawk","text":"Depending on the downstream application you want to import either Tomahawk representations of sequence variant files ( .twk ) or Tomahawk-generated output LD data ( .two ). Both operations require minimal work. First we will import a vcf / vcf.gz / bcf file into the binary Tomahawk file format ( .twk ): 1 twk <- import ( \"ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\" , \"1kgp3_chr6\" ) Auto-completion of file extensions You do not have to append the .twk suffix to the output name as Tomahawk will automatically add this if missing. In the example above \"1kgp3_chr6\" will be converted to \"1kgp3_chr6.two\" automatically. This is true for most Tomahawk commands when using the CLI but not when using rtomahawk . Unsafe method In the current release of rtomahawk , this subroutine does not check for user-interruption (for example Ctrl+C or Ctrl+Z ) commands. This means that you need to wait until the underlying process has finished or you have to terminate the host R session, in turn killing the spawned process. This will be fixed in upcoming releases. By default, rtomahawk will print verbose output to the console during the importing procedure. This will take several minutes depending on your system. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [2019-01-21 10:42:59,178][LOG][READER] Opening ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz... [2019-01-21 10:42:59,188][LOG][VCF] Constructing lookup table for 86 contigs... [2019-01-21 10:42:59,188][LOG][VCF] Samples: 2,504... [2019-01-21 10:42:59,188][LOG][WRITER] Opening 1kgp3_chr6.twk... [2019-01-21 10:44:41,225][LOG] Duplicate site dropped: 6:18233985 [2019-01-21 10:48:06,492][LOG] Duplicate site dropped: 6:55137646 [2019-01-21 10:49:01,472][LOG] Duplicate site dropped: 6:67839893 [2019-01-21 10:49:36,487][LOG] Duplicate site dropped: 6:74373442 [2019-01-21 10:49:55,386][LOG] Duplicate site dropped: 6:77843171 [2019-01-21 10:53:47,992][LOG] Duplicate site dropped: 6:121316830 [2019-01-21 10:56:08,853][LOG] Duplicate site dropped: 6:148573620 [2019-01-21 10:56:25,301][LOG] Duplicate site dropped: 6:151397786 [2019-01-21 10:58:16,584][LOG] Wrote: 4,784,608 variants to 9,570 blocks... [2019-01-21 10:58:16,584][LOG] Finished: 15m17,405s [2019-01-21 10:58:16,584][LOG] Filtered out 239,511 sites (4.76722%): [2019-01-21 10:58:16,584][LOG] Invariant: 15,485 (0.308213%) [2019-01-21 10:58:16,584][LOG] Missing threshold: 0 (0%) [2019-01-21 10:58:16,584][LOG] Insufficient samples: 0 (0%) [2019-01-21 10:58:16,584][LOG] Mixed ploidy: 0 (0%) [2019-01-21 10:58:16,584][LOG] No genotypes: 0 (0%) [2019-01-21 10:58:16,584][LOG] No FORMAT: 0 (0%) [2019-01-21 10:58:16,584][LOG] Not biallelic: 26,277 (0.523017%) [2019-01-21 10:58:16,584][LOG] Not SNP: 194,566 (3.87264%) The import procedure will return a new empty twk class with a file pointer set to the newly created output file: 1 2 > twk An object of class twk Importing variant call archives Success: This object is now ready to be used by other downstream functions in rtomahawk .","title":"Import files into Tomahawk"},{"location":"r-tutorial/#loading-and-reading-two-data","text":"Many functions in rtomahawk computes or use linkage-disequilibrium and require a different loading procedure involving the openTomahawkOuput subroutine. In this example we have a pre-computed .two output file available locally. We open this file and load all supportive information such as headers, footers, sample information, and validate the archive and collate this information in a new twk class together with a file pointer to the target archive: 1 twk <- openTomahawkOutput ( \"example.two\" ) Auto-completion of file extensions This function will not search for files in the directly by auto-completing missing file extensions. This is done purposely to avoid enforcing file extensions and any possible capitalization issues. This procedure is extremely fast as very little data is loaded from the archive and instead maintaining a data pointer. This approach, albeit a bit akward, enables us to handle more data than there is available system memory (RAM). 1 2 3 > system.time ( twk <- openTomahawkOutput ( \"example.two\" )) user system elapsed 0.003 0.000 0.002 Opening pre-computed LD data Success: This object is now ready to be used by other downstream functions in rtomahawk . In some situations you do want to load data into memory using the file handle pointer in a twk object. This is trivially done with the readRecords subroutine that require a twk class object as a required parameter. As it is very simple to accidentally load hundreds of millions of records if you are not careful we introduced the logical flag really . By default this flag is set to FALSE and will force terminate the loading procedure at 10 million records. 1 y <- readRecords ( twk , really = FALSE ) If the safety limit is reached a warning message will be printed to the console: 1 2 > y <- readRecords ( twk , really = FALSE ) limit reached Source of memory inefficiency In the current release, this subroutine load records (C structs) into vectors of records. This is unlike the required column-store format used in R . Because of this, we perform an implicit transposition of the internal structs into appropriate std::vectors followed by convertion into the appropriate data.table format. During this final convertion into SEXP structures, Rcpp will perform an unneccessary copy of the data resulting in a transient use (spike) of excess memory. We will address this problem in upcoming releases. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 > y An object of class twk Has internal data: 10000000 records FLAG ridA posA ridB posB REFREF REFALT ALTREF ALTALT D 1 11 6 123695 6 227960 4978 23 0 7 0.001389390 2 3 6 143500 6 228257 4425 387 78 118 0.019615739 3 3 6 144397 6 224822 4856 16 98 38 0.007295037 4 3 6 144398 6 224822 4856 16 98 38 0.007295037 5 3 6 150279 6 225420 4229 345 233 201 0.030687481 Dprime R R2 P ChiSqFisher ChiSqModel 1 1.0000000 0.4819338 0.2322602 1.304179e-16 1163.1592 0 2 0.5574107 0.3359277 0.1128474 5.789946e-71 565.1399 0 3 0.6954327 0.4345684 0.1888497 1.786734e-49 945.7594 0 4 0.6954327 0.4345684 0.1888497 1.786734e-49 945.7594 0 5 0.3974391 0.3499740 0.1224818 8.508703e-90 613.3890 0 ... FLAG ridA posA ridB posB REFREF REFALT ALTREF ALTALT D 9999995 3 6 1834075 6 1888336 4932 4 52 20 0.003924711 9999996 11 6 1834098 6 1885227 4892 0 97 19 0.003706051 9999997 11 6 1834098 6 1886103 4982 0 7 19 0.003774233 9999998 11 6 1834098 6 1890443 4873 0 116 19 0.003691657 9999999 11 6 1834098 6 1890998 4873 0 116 19 0.003691657 10000000 11 6 1834098 6 1891303 4873 0 116 19 0.003691657 Dprime R R2 P ChiSqFisher ChiSqModel 9999995 0.8309022 0.4774060 0.2279165 8.176940e-35 1141.4056 0 9999996 1.0000000 0.4007599 0.1606085 1.856439e-32 804.3274 0 9999997 1.0000000 0.8542505 0.7297439 4.211277e-48 3654.5574 0 9999998 1.0000000 0.3707673 0.1374684 4.173075e-31 688.4415 0 9999999 1.0000000 0.3707673 0.1374684 4.173075e-31 688.4415 0 10000000 1.0000000 0.3707673 0.1374684 4.173075e-31 688.4415 0 A more useful procedure is to slice out regions of interest that match some set of criteron. Such as for example retrieving records from the region chr6:5e6-10e6 with an R2 value between 0.4 and 0.8 and a P-value < 0.001: 1 2 f <- setFilters ( minR2 = 0.6 , maxR2 = 0.8 , maxP = 0.001 ) recs <- readRecords ( twk , \"6:5e6-10e6\" , filters = f , really = TRUE ) Interval slicing Intervals are left-inclusive and right-exclusive [A, B) and use 1-base coordinates. The interval syntax has to match either: A) contig , B) contig:pos , or C) contig:pos-pos . It is important to note that interval slicing is performed on the forward position only ( posA ) and that by default the output data generarted from Tomahawk have bidirectional symmetry such that two tuples (A,B) == (B,A), exists and are mirrored. We can ascertain that the filtering procedure work correctly by looking at the summary statistics for each column and note that posA is bounded by 5Mb-10Mb, R2 is bounded by [0.6, 0.8], and P < 0.001: 1 summary ( recs ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 Min. 1st Qu. Median Mean 3rd Qu. FLAG 3.000000e+00 3.000000e+00 3.000000e+00 5.311864e+01 3.000000e+00 ridA 3.659840e+05 3.659840e+05 3.659840e+05 3.659840e+05 3.659840e+05 posA 5.000012e+06 7.093750e+06 7.887604e+06 7.780695e+06 8.673979e+06 ridB 3.659840e+05 3.659840e+05 3.659840e+05 3.659840e+05 3.659840e+05 posB 2.869352e+06 7.094321e+06 7.887945e+06 7.782022e+06 8.675012e+06 REFREF 0.000000e+00 2.801000e+03 4.728000e+03 3.836738e+03 4.953000e+03 REFALT 0.000000e+00 2.000000e+00 1.800000e+01 2.416587e+02 1.300000e+02 ALTREF 0.000000e+00 3.000000e+00 2.600000e+01 2.241404e+02 1.630000e+02 ALTALT 0.000000e+00 3.500000e+01 1.630000e+02 7.054628e+02 1.076000e+03 D -2.231411e-01 4.564901e-03 2.287482e-02 5.856900e-02 1.283542e-01 Dprime 7.749466e-01 8.886666e-01 9.506363e-01 9.368918e-01 9.961449e-01 R 7.745978e-01 8.045203e-01 8.354463e-01 8.354627e-01 8.654019e-01 R2 6.000017e-01 6.472530e-01 6.979705e-01 6.992205e-01 7.489204e-01 P 0.000000e+00 0.000000e+00 2.578244e-297 2.630748e-12 2.487910e-85 ChiSqFisher 3.004809e+03 3.241443e+03 3.495436e+03 3.501696e+03 3.750593e+03 ChiSqModel 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 Max. FLAG 2.063000e+03 ridA 3.659840e+05 posA 9.999976e+06 ridB 3.659840e+05 posB 1.240232e+07 REFREF 5.003000e+03 REFALT 4.945000e+03 ALTREF 5.005000e+03 ALTALT 4.999000e+03 D 2.233769e-01 Dprime 1.000000e+00 R 8.944252e-01 R2 7.999965e-01 P 2.392816e-07 ChiSqFisher 4.006383e+03 ChiSqModel 0.000000e+00 Opening and slicing LD data Success: This object is now ready to be used by other downstream functions in rtomahawk . There are a large number of filter parameters available to slice out the exact data of interest: Field Type flagInclude integer flagExclude integer minR numeric maxR numeric minR2 numeric maxR2 numeric minD numeric maxD numeric minDprime numeric maxDprime numeric minP numeric maxP numeric minP1 numeric maxP1 numeric minP2 numeric maxP2 numeric minQ1 numeric maxQ1 numeric minQ2 numeric maxQ2 numeric minChiSqFisher numeric maxChiSqFisher numeric minChiSqModel numeric maxChiSqModel numeric upperOnly logical lowerOnly logical See ?setFilters for more information.","title":"Loading and reading .two data"},{"location":"r-tutorial/#calculating-linkage-disequilibrium","text":"","title":"Calculating linkage-disequilibrium"},{"location":"r-tutorial/#plotting-data-with-rtomahawk","text":"","title":"Plotting data with rtomahawk"},{"location":"r-tutorial/#color-schemes","text":"rtomahawk comes prepacked with a number of color schemes. Most of these are taken directly from the viridis R package and are included here independent of that package to remove the package interdependency. These color schemes are callable as functions taking as require arguments the number of distinct values ( n ) and optionally the desired opacity (alpha) level ( alpha ). Color scheme viridis plasma magma inferno cividis default We can plot these values as a gradient of values from [0, 100) and as [0, 11] by calling the displayColors function: 1 displayColors ()","title":"Color schemes"},{"location":"r-tutorial/#square-representation","text":"Graphically representing LD data is useful for checking data quality and in exploration. If your data has already been loaded into memory using readRecords it is possible to plot this data as individual data points using the plotLD function. This approach is generally only feasable for visualizing smaller genomic regions, for example, < 2-3 megabases. If you are investigating longer ranges than this you should consider using the aggregation functions provided in tomahawk / rtomahawk . Because of the vast number of data points rendered and the finite amount of pixels available, we render data points with an opacity gradient scaled according to its R2 value from [0.1, 1]. This allows for mixing of both colors and opacities to more clearly represent the distribution of the underlying data. It is possible to disable this functionality by setting the optional argument opacity to FALSE . In the following examples, we render both a large region (5-8 Mb) and a small region (5.0-5.6 Mb) with and without the opacity flag set. We also showcase the different color schemes. 1 2 3 4 5 6 7 8 9 10 11 # Load some local data into memory. twk <- openTomahawkOutput ( \"1kgp3_chr6.two\" ) y <- readRecords ( twk , \"6:5e6-10e6\" , really = TRUE ) # Plot two panels in the same figure. # Top panel: opacity gradient from [0.1, 1.0] mapping to # R2 range [0.1, 0.1, 0.2, ..., 1.0] # <COLOR> placeholder gets replaced with one of the color # schemes described below. par ( mfrow = c ( 2 , 1 )) plotLD ( y , ylim = c ( 5e6 , 8e6 ), xlim = c ( 5e6 , 8e6 ), colors =< COLOR > , bg =< COLOR > ( 11 ) [1] ) plotLD ( y , ylim = c ( 5e6 , 8e6 ), xlim = c ( 5e6 , 8e6 ), colors =< COLOR > , bg =< COLOR > ( 11 ) [1] , opacity = FALSE ) Color scheme Image default cividis inferno magma plasma viridis By default, the birectionally symmetric output data from tomahawk is kept (i.e. both (A,B) and (B,A)) tuples are kept. For this reason, the output plot will be square (or rectangular with mismatched xlim and ylim parameters). In some cases, only the upper or lower triangular values are desired. We can control what values are plotted with the logical upper and lower parameters. 1 2 3 4 par ( mfrow = c ( 1 , 3 )) plotLD ( y , ylim = c ( 5e6 , 8e6 ), xlim = c ( 5e6 , 8e6 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] ) plotLD ( y , ylim = c ( 5e6 , 8e6 ), xlim = c ( 5e6 , 8e6 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , upper = T ) plotLD ( y , ylim = c ( 5e6 , 8e6 ), xlim = c ( 5e6 , 8e6 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , lower = T )","title":"Square representation"},{"location":"r-tutorial/#triangular-representation","text":"In many cases, there is generally no need to graphically represent the entire square (or rectangular) symmetric matrix of associations. This is especially true when combining multiple graphs together to create a more comprehensive picture of a particular region or feature. The topic of combining plots will be explored in greter detail below. All of the examples here involves the subroutine plotLDTriangular . As a design choice, we decided to restrict the rendered y-axis data such that it is always bounded by the x-axis limits. For this reason, these plots will always be triangular will partially \"missing\" (omitted) values even if they are technically present in the dataset. First, we describe its most basic use case. In the following example, we will plot a section of associations with the viridis color palette and specifiy the background to be the lowest (in this case, first) color in that scheme. 1 plotLDTriangular ( y , colors = viridis ( 11 ), bg = viridis ( 11 ) [1] ) Because of the smallish haplotype blocks in humans, most of these visible triangular structures will have limited span in the y-axis. We can truncate the y-axis to zoom into the local neighbourhood and more accurately display the local haplotype structure. 1 plotLDTriangular ( y , colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , ylim = c ( 0 , 300e3 )) It is possible to control the orientation (rotation) of the output graph by specifying the orientation parameter. The numerial encodings are: 1) standard; 2) upside down; 3) left-right flipped; and 4) right-left flipped. 1 2 3 4 5 par ( mfrow = c ( 2 , 2 )) plotLDTriangular ( y , ylim = c ( 0 , 300e3 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 1 ) plotLDTriangular ( y , ylim = c ( 0 , 300e3 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 2 ) plotLDTriangular ( y , ylim = c ( 0 , 300e3 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 3 ) plotLDTriangular ( y , ylim = c ( 0 , 300e3 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 4 ) It is possible to completely disable all anotation by setting the logical parameter annotate to FALSE . This will remove titles, axes, and ticks. 1 plotLDTriangular ( y , ylim = c ( 0 , 300e3 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 1 , annotate = FALSE ) Note that these plotting functions respect the global mar (margin) values and (by default) will have white space around it. We can change the global par argument to, for example, zero to remove these margins when annotation is disabled for edge-to-edge graphics.","title":"Triangular representation"},{"location":"r-tutorial/#visualizing-gwas-data-and-ld","text":"In this section we will produce LocusZoom-like plots using GWAS data from the UK BioBank comprised exclusively of white British individuals. The Roslin Institute at the University of Edinbrugh host a data browser of associatins called Gene Atlas . In the following examples we will investigate the association of genotypes at chromosome 6 and diabetes in this cohort. Data in its entirety can be explored further using the Gene Atlas. To reproduce the results below download the imputed data for chromosome 6 and the associated positional information . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Downloaded data from: # http://static.geneatlas.roslin.ed.ac.uk/gwas/allWhites/imputed/data.copy/imputed.allWhites.selfReported_n_1245.chr6.csv.gz # http://static.geneatlas.roslin.ed.ac.uk/gwas/allWhites/snps/extended/snps.imputed.chr6.csv.gz library ( data.table ) # For speedier reading of data. # We zcat (uncompress gzipped archive) directly into fread. # This may not work for Windows users. In that case, decompress # the files manually first. x <- fread ( \"zcat imputed.allWhites.selfReported_n_1245.chr6.csv.gz\" , sep = \" \" ) snp <- fread ( \"zcat snps.imputed.chr6.csv.gz\" , sep = \" \" ) # Keep matching data available in both files. We neeed to maintain # parity between the two sets. snp <- snp [match ( x $ SNP , snp $ SNP ), ] # Transform linear-scale (untransformed) P-values into -log10(P). snp $ p <- - log10 ( x $ `PV-selfReported_n_1245` ) # Setup path to local Tomahawk file to compute LD against. twk2 <- new ( \"twk\" ) twk2 @ file.path <- \"1kgp3_chr6.twk\" # Load human recombination data (hg19) supplied with `rtomahawk` data ( gmap ) # Region of 1 Mb in either direction of target. single <- plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 1e6 , minR2 = 0 ) # Region of 50 Kb in either direction of target. single <- plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 50e3 , minR2 = 0 ) Internal LD computation The plotLZ plotting function will internally compute linkage-disequilibrium for a target SNV and it's surrounding genomic region. This background computation is stored in a temporary file and then loaded back into memory and returned as a new twk class instance. If you have further use of this information then you need to capture the return value of this function. It is possible to change the default temp directory used in R with the subroutine set.tempdir . Please note that all data written to this temporary directory will be purged upon exiting the current session of R . These functions are extremely fast as the R-bindings use .Call commands to communicate with the compiled C++ shared object: 1 2 3 4 5 6 7 > system.time ( plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 1e6 , minR2 = 0 )) user system elapsed 1.405 0.001 1.407 > system.time ( plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 50e3 , minR2 = 0 )) user system elapsed 0.115 0.004 0.119 Almost all of this time is spent rendering symbols in the vectorized plot. The same command using the CLI takes roughly 1/3rd of the time: 1 2 3 4 $ time tomahawk scalc -i 1kgp3_chr6.twk -I 6 :20694884 -w 1000000 > /dev/null real 0m0.452s user 0m0.937s sys 0m0.061s Many aspects of the plots can be customized to your needs. For example, modifying the datapoint symbol pch in the valid range [21, 25]. Note that all other pch values are technically valid but will not generate a fill color. If your visualizations require additional data layers it is possible to combine these into a single plot as rtomahawk renders plots using base-R. In this example we will combine two plots generated by rtomahawk : the GWAS P-value and its single-site LD together with the all-vs-all pairwise LD for the same region. 1 2 3 4 5 6 par ( mfrow = c ( 2 , 1 ), mar = c ( 0 , 5 , 3 , 5 )) # Set bottom margin to 0. single <- plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 1e6 , minR2 = 0 , xlab = \"\" , xaxt = \"n\" ) twk <- openTomahawkOutput ( \"test_region.two\" ) y <- readRecords ( twk , really = TRUE ) par ( mar = c ( 5 , 5 , 0 , 5 )) # Set top margin to 0. plotLDTriangular ( y , ylim = c ( 0 , 500e3 ), xlim = c ( 20694884 - 1e6 , 20694884 + 1e6 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 2 , cex = .25 , main = \"\" ) We can zoom into the local region (100kb flanking region) by simply changing the window parameter in plotLZ and the xlim range in plotLDTriangular . It is possible to add more advanced data layers using external packages with some simple manipulations. In this example we will add a gene track using genetic information extracted from biomaRt and drawn using Sushi , both third-party packages. biomaRt information The R package biomaRt requires internet connectivity to function as the package itself is a wrapper around the Ensembl BioMart tool that allows extraction of connected data from databases without having to perform explicit programming. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 library ( biomaRt ) mart <- useMart ( host = 'http://grch37.ensembl.org' , biomart = \"ensembl\" , dataset = \"hsapiens_gene_ensembl\" ) # We will retrieve results from biomaRt twice because the database # does _not_ allow simultaneous queries for exon-level data and # gene-level data. To overcome this problem, we will perform two # quries to the database and merge the results together. results <- getBM ( attributes = c ( \"ensembl_exon_id\" , \"exon_chrom_start\" , \"exon_chrom_end\" ), filters = c ( \"chromosome_name\" , \"start\" , \"end\" ), values = list ( 6 , 20694884-1e6 , 20694884+1e6 ), mart = mart ) results2 <- getBM ( attributes = c ( \"ensembl_exon_id\" , \"hgnc_symbol\" , \"chromosome_name\" , \"start_position\" , \"end_position\" , \"strand\" ), filters = c ( \"chromosome_name\" , \"start\" , \"end\" ), values = list ( 6 , 20694884-1e6 , 20694884+1e6 ), mart = mart ) results <- cbind ( results , results2[ , -1 , drop = F ] ) results <- results[results $ hgnc_symbol != \"\" , ] results <- results[ , c ( 2 , 3 , 4 , 5 , 8 ) ] names ( results ) <- c ( \"start\" , \"stop\" , \"gene\" , \"chrom\" , \"strand\" ) results $ score = \".\" results <- results[ , c ( \"chrom\" , \"start\" , \"stop\" , \"gene\" , \"score\" , \"strand\" ) ] library ( Sushi ) chrom = 6 chromstart = 20694884-1e6 chromend = 20694884+1e6 layout ( matrix ( 1 : 3 , ncol = 1 ), heights = c ( 2 , 2 , 1 )) par ( mar = c ( 0 , 5 , 3 , 5 )) # Set bottom margin to 0. single <- plotLZ ( twk2 , \"6:20694884\" , snp , gmap , window = 1e6 , minR2 = 0 , xlab = \"\" , xaxt = \"n\" ) twk <- openTomahawkOutput ( \"test_region.two\" ) y <- readRecords ( twk , really = TRUE ) par ( mar = c ( 0 , 5 , 0 , 5 )) # Set top and bottom margins to 0. plotLDTriangular ( y , ylim = c ( 0 , 500e3 ), xlim = c ( 20694884 - 1e6 , 20694884 + 1e6 ), colors = viridis ( 11 ), bg = viridis ( 11 ) [1] , orientation = 2 , cex = .25 , main = \"\" ) par ( mar = c ( 2 , 5 , 0 , 5 )) plotGenes ( geneinfo = results , chrom = chrom , chromstart = chromstart , chromend = chromend , labeloffset = .5 , fontsize = 1 , arrowlength = 0.025 ) abline ( v = 20694884 , lty = \"dashed\" , col = \"grey\" ) # Highjack internal `rtomahawk` function for drawing genomic axes. # This function is unexported and is not intended for general use. rtomahawk ::: addGenomicAxis ( c ( chromstart , chromend ), at = 1 , las = 1 , F ) Again, we can zoom into the local region (200kb flanking region) by simply changing the appropriate parameters in the code above. The plotting time can vary from milliseconds to several seconds depending on the number of points that are being computed directly (top panel) and the number of points that are loaded and rendered (middle panel). In the first example above, rtomahawk and tomahawk computes and plots millions of LD associations and data points in a few seconds on a single thread. 1 2 3 > system.time ( f ()) user system elapsed 6.336 0.260 6.233 In this code snippet, the function f() , is simply a placeholder for the code above.","title":"Visualizing GWAS data and LD"},{"location":"r-tutorial/#aggregating-and-visualizing-datasets","text":"Quantile-normalized (left) or linear range (right) Unsafe method In the current release of rtomahawk , this subroutine does not check for user-interruption (for example Ctrl+C or Ctrl+Z ) commands. This means that you need to wait until the underlying process has finished or you have to terminate the host R session, in turn killing the spawned process. This will be fixed in upcoming releases. 1 2 3 4 5 twk <- openTomahawkOutput ( \"example.two\" ) x <- aggregate ( twk , aggregation = \"r2\" , reduction = \"count\" , xbins = 1000 , ybins = 1000 , minCount = 50 , verbose = T , threads = 8 ) par ( mar = c ( 5 , 5 , 5 , 8 ), mfrow = c ( 1 , 2 )) plotAggregation ( x , normalize = TRUE ) plotAggregation ( x , normalize = FALSE ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 > agg <- aggregate ( twk , \"r2\" , \"count\" , 1000 , 1000 , 50 , verbose = T , threads = 6 ) Program : tomahawk -062954 eb ( Tools for computing , querying and storing LD data ) Libraries : tomahawk -0.7.0 ; ZSTD -1.3.1 ; htslib 1.9 Contact : Marcus D. R. Klarqvist < mk819 @ cam.ac.uk > Documentation : https :// github.com / mklarqvist / tomahawk License : MIT ---------- [2019 -01-19 11 : 05 : 37 , 072 ][LOG] Calling aggregate... [2019 -01-19 11 : 05 : 37 , 072 ][LOG] Performing 2 - pass over data... [2019 -01-19 11 : 05 : 37 , 072 ][LOG] ===== First pass ( peeking at landscape ) ===== [2019 -01-19 11 : 05 : 37 , 072 ][LOG] Blocks : 32 [2019 -01-19 11 : 05 : 37 , 072 ][LOG] Uncompressed size : 5.286261 Gb [2019 -01-19 11 : 05 : 37 , 072 ][LOG][THREAD] Data / thread : 881.043564 Mb [2019 -01-19 11 : 05 : 41 , 040 ][LOG] ===== Second pass ( building matrix ) ===== [2019 -01-19 11 : 05 : 41 , 040 ][LOG] Aggregating 49 , 870 , 388 records... [2019 -01-19 11 : 05 : 41 , 040 ][LOG][THREAD] Allocating : 240.000000 Mb for matrices... [2019 -01-19 11 : 05 : 44 , 797 ][LOG] Aggregated 49 , 870 , 388 records in 1 , 000 , 000 bins. [2019 -01-19 11 : 05 : 44 , 798 ][LOG] Finished. Load a pre-computed tomahawk aggregation object: 1 agg <- loadAggregate ( \"agg.twa\" )","title":"Aggregating and visualizing datasets"},{"location":"tutorial/","text":"Getting started with tomahawk \u00b6 About \u00b6 This is an introductory tutorial for using Tomahawk. It will cover: Importing data into Tomahawk Computing linkage-disequilibrium Subsetting and filtering output data Aggregating datasets Usage instructions \u00b6 The CLI of Tomahawk comprises of several distinct subroutines (listed below). Executing tomahawk gives a list of commands with brief descriptions and tomahawk <command> gives detailed details for that command. All primary Tomahawk commands operate on the binary Tomahawk twk and Tomahawk output two file format. Interconversions between twk and vcf / bcf is supported through the commands import for vcf / bcf -> twk and view for twk -> vcf . Linkage disequilibrium data is written out in binary two format or human-readable ld format. Command Description aggregate data rasterization framework for TWO files calc calculate linkage disequilibrium scalc calculate linkage disequilibrium for a single site concat concatenate TWO files from the same set of samples import import VCF / VCF.gz / BCF to TWK sort sort TWO file view TWO -> LD / TWO view, TWO subset and filter haplotype extract per-sample haplotype strings in FASTA /binary format relationship compute marker-based pair-wise sample relationship matrices decay compute LD-decay over distance prune perform graph-based LD-pruning of variant sites Importing into Tomahawk \u00b6 By design Tomahawk only operates on diploid and bi-allelic SNVs and as such filters out indels and complex variants. Tomahawk does not support mixed phasing of genotypes in the same variant (e.g. 0|0 , 0/1 ). If mixed phasing is found for a record, all genotypes for that site are converted to unphased genotypes. This is a conscious design choice as this will internally invoke the correct algorithm to use for mixed-phase cases. Importing standard files to Tomahawk involes using the import command. The following command imports a bcf file and outputs file.twk while filtering out variants with >20% missingness and sites that deviate from Hardy-Weinberg equilibrium with a probability < 0.001. 1 tomahawk import -i file.bcf -o file -m 0 .2 -H 1e-3 1 2 $ md5sum 1kgp3_chr6.bcf 8c05554ebe1a51e99be2471c96fad4d9 1kgp3_chr6.bcf 1 2 $ bcftools view 1kgp3_chr6.bcf -HG | wc -l 5024119 1 $ time tomahawk import -i 1kgp3_chr6.bcf -o 1kgp3_chr6 Auto-completion of file extensions You do not have to append the .twk suffix to the output name as Tomahawk will automatically add this if missing. In the example above \"1kgp3_chr6\" will be converted to \"1kgp3_chr6.twk\" automatically. This is true for most Tomahawk commands when using the CLI. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Program: tomahawk-264d039a-dirty (Tools for computing, querying and storing LD data) Libraries: tomahawk-0.7.0; ZSTD-1.3.8; htslib 1.9 Contact: Marcus D. R. Klarqvist <mk819@cam.ac.uk> Documentation: https://github.com/mklarqvist/tomahawk License: MIT ---------- [2019-01-21 11:58:15,692][LOG] Calling import... [2019-01-21 11:58:15,692][LOG][READER] Opening 1kgp3_chr6.bcf... [2019-01-21 11:58:15,695][LOG][VCF] Constructing lookup table for 86 contigs... [2019-01-21 11:58:15,695][LOG][VCF] Samples: 2,504... [2019-01-21 11:58:15,695][LOG][WRITER] Opening 1kgp3_chr6.twk... [2019-01-21 11:58:39,307][LOG] Duplicate site dropped: 6:18233985 [2019-01-21 11:59:26,749][LOG] Duplicate site dropped: 6:55137646 [2019-01-21 11:59:39,315][LOG] Duplicate site dropped: 6:67839893 [2019-01-21 11:59:47,176][LOG] Duplicate site dropped: 6:74373442 [2019-01-21 11:59:51,440][LOG] Duplicate site dropped: 6:77843171 [2019-01-21 12:00:41,784][LOG] Duplicate site dropped: 6:121316830 [2019-01-21 12:01:12,830][LOG] Duplicate site dropped: 6:148573620 [2019-01-21 12:01:16,557][LOG] Duplicate site dropped: 6:151397786 [2019-01-21 12:01:42,644][LOG] Wrote: 4,784,608 variants to 9,570 blocks... [2019-01-21 12:01:42,644][LOG] Finished: 03m26,952s [2019-01-21 12:01:42,644][LOG] Filtered out 239,511 sites (4.76722%): [2019-01-21 12:01:42,645][LOG] Invariant: 15,485 (0.308213%) [2019-01-21 12:01:42,645][LOG] Missing threshold: 0 (0%) [2019-01-21 12:01:42,645][LOG] Insufficient samples: 0 (0%) [2019-01-21 12:01:42,645][LOG] Mixed ploidy: 0 (0%) [2019-01-21 12:01:42,645][LOG] No genotypes: 0 (0%) [2019-01-21 12:01:42,645][LOG] No FORMAT: 0 (0%) [2019-01-21 12:01:42,645][LOG] Not biallelic: 26,277 (0.523017%) [2019-01-21 12:01:42,645][LOG] Not SNP: 194,566 (3.87264%) Computing linkage-disequilibrium \u00b6 All pairwise comparisons \u00b6 In this first example, we will compute all-vs-all LD associations using the data we imported in the previous section. To limit compute, we restrict our attention to a 1/45 section of the data by passing the -c and -C job parameters. We will be using 8 threads ( -t ), but you may need to modify this to match the hardware available on your host machine. This job involves comparing a pair of variants >141 billion times and as such takes around 30 min to finish on most machines. 1 $ tomahawk calc -pi 1kgp3_chr6.twk -o 1kgp3_chr6_1_45 -C 1 -c 45 -t 8 Valid job balancing partitions When computing genome-wide LD the balancing requires that number of sub-problems ( -c ) must be a member of the function: $$ \\binom{c}{2} + c = \\frac{c^2 + c}{2}, c > 0 $$ This function is equivalent to the upper-triangular of a square ( c -by- c ) matrix plus the diagonal. Read more about load partitioning in Tomahawk. Here is the first 100 valid partition sizes: 1 , 3 , 6 , 10 , 15 , 21 , 28 , 36 , 45 , 55 , 66 , 78 , 91 , 105 , 120 , 136 , 153 , 171 , 190 , 210 , 231 , 253 , 276 , 300 , 325 , 351 , 378 , 406 , 435 , 465 , 496 , 528 , 561 , 595 , 630 , 666 , 703 , 741 , 780 , 820 , 861 , 903 , 946 , 990 , 1035 , 1081 , 1128 , 1176 , 1225 , 1275 , 1326 , 1378 , 1431 , 1485 , 1540 , 1596 , 1653 , 1711 , 1770 , 1830 , 1891 , 1953 , 2016 , 2080 , 2145 , 2211 , 2278 , 2346 , 2415 , 2485 , 2556 , 2628 , 2701 , 2775 , 2850 , 2926 , 3003 , 3081 , 3160 , 3240 , 3321 , 3403 , 3486 , 3570 , 3655 , 3741 , 3828 , 3916 , 4005 , 4095 , 4186 , 4278 , 4371 , 4465 , 4560 , 4656 , 4753 , 4851 , 4950 , 5050 Massive workload By default, Tomahawk will attempt to compute the genome-wide linkage-disequilibrium for all the provided variants and samples. This generally require huge amount of pairwise variant comparisons. Depending on your interests, you may not want to undertake this large compute and need to parameterize for your particular use-case. Different load balancing As a consequence of the job balancing approach in Tomahawk, the diagonal jobs will always involve less variant comparisons (\\( \\binom{n}{2} \\)) compared to off-diagonal jobs (\\(n \\times m\\)), where \\(n\\) and \\(m\\) are the number of records in either block. Keep this in mind if you are choosing a partition size given the run-times of a given job. For example, examine the total run-time of the second job ( -C 2 ) \u2014 the first off-diagonal job \u2014 as a proxy for the expected average run-time per job. Output ordering By design, Tomahawk computes pairwise associations using an out-of-order execution paradigm and further permutes the input ordering by using cache-blocking . Additionally, each thread/slave transiently store a number of computed associations in a private buffer that will be flushed upon reaching some frequency threshold. Because of these technicalities, Tomahawk will neither produce ordered output nor will the permutation of the output order be identical between runs on identical data. This has no practical consequence to most downstream applications in Tomahawk with the exception of query speeds. To address this limitation we have introduced a powerful sorting paradigm that will be discussed below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Program: tomahawk-264d039a-dirty (Tools for computing, querying and storing LD data) Libraries: tomahawk-0.7.0; ZSTD-1.3.8; htslib 1.9 Contact: Marcus D. R. Klarqvist <mk819@cam.ac.uk> Documentation: https://github.com/mklarqvist/tomahawk License: MIT ---------- [2019-01-21 12:13:50,210][LOG] Calling calc... [2019-01-21 12:13:50,211][LOG][READER] Opening 1kgp3_chr6.twk... [2019-01-21 12:13:50,212][LOG] Samples: 2,504... [2019-01-21 12:13:50,212][LOG][BALANCING] Using ranges [0-1063,0-1063] in square mode... [2019-01-21 12:13:50,212][LOG] Allocating 1,063 blocks... [2019-01-21 12:13:50,213][LOG] Running in standard mode. Pre-computing data... [2019-01-21 12:13:50,213][LOG][SIMD] Vectorized instructions available: SSE4... [2019-01-21 12:13:50,213][LOG] Constructing list, vector, RLE... [2019-01-21 12:13:50,213][LOG][THREAD] Unpacking using 7 threads: ....... Done! 01,291s [2019-01-21 12:13:51,504][LOG] 531,500 variants from 1,063 blocks... [2019-01-21 12:13:51,504][LOG][PARAMS] square=TRUE,window=FALSE,low_memory=FALSE,bitmaps=FALSE,single=FALSE,force_phased=TRUE,force_unphased=FALSE,compression_level=1,block_size=500,output_block_size=10000,l_surrounding=500000,minP=1.000000,minR2=0.100000,maxR2=100.000000,minDprime=0.000000,maxDprime=100.000000,n_chunks=45,c_chunk=0,n_threads=8,ldd_type=3,cycle_threshold=0 [2019-01-21 12:13:51,504][LOG] Performing: 141,245,859,250 variant comparisons... [2019-01-21 12:13:51,504][LOG][WRITER] Opening 1kgp3_chr6_1_45.two... [2019-01-21 12:13:51,505][LOG][THREAD] Spawning 8 threads: ........ [2019-01-21 12:13:51,533][PROGRESS] Time elapsed Variants Genotypes Output Progress Est. Time left [2019-01-21 12:14:21,533][PROGRESS] 30,000s 3,685,746,500 9,229,109,236,000 988,858 2.60945% 18m39s [2019-01-21 12:14:51,533][PROGRESS] 01m00,000s 6,425,868,750 16,090,375,350,000 1,689,422 4.54942% 20m58s [truncated] [2019-01-21 12:39:51,546][PROGRESS] 26m00,013s 140,131,382,750 350,888,982,406,000 48,635,114 99.211% 12s [2019-01-21 12:40:04,317][PROGRESS] Finished in 26m12,784s. Variants: 141,245,859,250, genotypes: 353,679,631,562,000, output: 49,870,388 [2019-01-21 12:40:04,317][PROGRESS] 89,806,242 variants/s and 224,874,830,855 genotypes/s [2019-01-21 12:40:04,321][LOG][PROGRESS] All done...26m12,815s! This run generated 935817820 bytes (935.8 MB) of output data in binary format using the default compression level (1): 1 2 $ ls -l 1kgp3_chr6_1_45.two -rw-rw-r-- 1 mk819 mk819 935817820 Jan 21 12 :40 1kgp3_chr6_1_45.two If this data was stored in plain, human-readable, text format it would use over 45 GB: 1 2 $ tomahawk view -i 1kgp3_chr6_1_45.two | wc -c 4544702179 Sliding window \u00b6 If you are working with a species with well-know LD structure (such as humans) you can reduce the computational cost by limiting the search-space to a fixed-sized sliding window ( -w ). In window mode you are free to choose any arbitrary sub-problem ( -c ) size. Window properties By default, Tomahawk computes the linkage disequilibrium associations for all pairs of variants within -w bases of the target SNV on either direction. In order to maximize computational throughput, we made the design decision to maintain blocks of data that overlap to the target interval. This has the consequence that adjacent data that may not directly overlap with the target region will still be computed. The technical reasoning for this is, in simplified terms, that the online repackaging of internal data blocks is more expensive than computing a small (relatively) number of non-desired (off-target) associations. 1 time tomahawk calc -pi 1kgp3_chr6.twk -o 1kgp3_chr6_4mb -w 4000000 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Program: tomahawk-264d039a-dirty (Tools for computing, querying and storing LD data) Libraries: tomahawk-0.7.0; ZSTD-1.3.8; htslib 1.9 Contact: Marcus D. R. Klarqvist <mk819@cam.ac.uk> Documentation: https://github.com/mklarqvist/tomahawk License: MIT ---------- [2019-01-21 14:10:33,616][LOG] Calling calc... [2019-01-21 14:10:33,616][LOG][READER] Opening 1kgp3_chr6.twk... [2019-01-21 14:10:33,619][LOG] Samples: 2,504... [2019-01-21 14:10:33,619][LOG][BALANCING] Using ranges [0-9570,0-9570] in window mode... [2019-01-21 14:10:33,619][LOG] Allocating 9,570 blocks... [2019-01-21 14:10:33,620][LOG] Running in standard mode. Pre-computing data... [2019-01-21 14:10:33,620][LOG][SIMD] Vectorized instructions available: SSE4... [2019-01-21 14:10:33,620][LOG] Constructing list, vector, RLE... [2019-01-21 14:10:33,620][LOG][THREAD] Unpacking using 7 threads: ....... Done! 15,774s [2019-01-21 14:10:49,394][LOG] 4,784,608 variants from 9,570 blocks... [2019-01-21 14:10:49,394][LOG][PARAMS] square=TRUE,window=TRUE,low_memory=FALSE,bitmaps=FALSE,single=FALSE,force_phased=TRUE,force_unphased=FALSE,compression_level=1,block_size=500,output_block_size=10000,window_size=4000000,l_surrounding=500000,minP=1.000000,minR2=0.100000,maxR2=100.000000,minDprime=0.000000,maxDprime=100.000000,n_chunks=1,c_chunk=0,n_threads=8,ldd_type=3,cycle_threshold=0 [2019-01-21 14:10:49,394][LOG] Performing: 11,446,234,464,528 variant comparisons... [2019-01-21 14:10:49,394][LOG][WRITER] Opening 1kgp3_chr6_4mb.two... [2019-01-21 14:10:49,402][LOG][THREAD] Spawning 8 threads: ........ [2019-01-21 14:10:49,424][PROGRESS] Time elapsed Variants Genotypes Output Progress Est. Time left [2019-01-21 14:11:19,424][PROGRESS] 30,000s 2,311,240,500 5,787,346,212,000 853,368 0 0 [2019-01-21 14:11:49,424][PROGRESS] 01m00,000s 4,562,731,500 11,425,079,676,000 1,717,956 0 0 [truncated] [2019-01-21 16:12:49,492][PROGRESS] 02h02m00,068s 527,889,726,500 1,321,835,875,156,000 470,354,846 0 0 [2019-01-21 16:13:17,236][PROGRESS] Finished in 02h02m27,812s. Variants: 529,807,522,528, genotypes: 1,326,638,036,410,112, output: 473,514,018 [2019-01-21 16:13:17,237][PROGRESS] 72,104,114 variants/s and 180,548,701,880 genotypes/s [2019-01-21 16:13:17,257][LOG][PROGRESS] All done...02h02m27,854s! Single interval vs its local neighbourhood \u00b6 In many cases we are not interested in computing large-scale linkage-disequilibrium associations but have a limited genomics window of interest. If this is the case, you can use the specialization subroutine of calc called scalc that is parameterizable with a target region and a neighbourhood size in bases. In this example, we will compute the regional associations of variants mapping to the interval chr6:10e6-10.1e6 and a 100kb neighbourhood. 1 tomahawk scalc -i 1kgp3_chr6.twk -o test -I 6 :10e6-10.1e6 -w 100000 Neighbourhood interval The neighbourhood is computed as the region from [start of interval - neighbourhood, start of interval) and (end of interval + neighbourhood). If the start of the interval minus the neighbourhood falls outside the chromosome (position < 0) then this interval is truncated to 0 in the left end. Scalability This subroutine was designed to be used with a relatively short interval to maximize computability in this case. It is however possible to provide any valid interval as the target region to compute linkage-disequilibrium for. Providing large intervals will result in poor performance and potentially undesired output. Concatenating multiple archives \u00b6 On of the immediate downsides of partitioning compute into multiple non-overlapping sub-problems is that we will generate a large number of independent files that must be merged prior to downstream analysis. Fortunatly, this is a trivial operation in Tomahawk and involves the concat command. First lets compute some data using the first 3 out of 990 partitions of the dataset used above. 1 for i in { 1 ..3 } ; do time tomahawk calc -pi 1kgp3_chr6.twk -c 990 -C $i -o part $i \\_ 3 .two ; done Next, since we only have three files, we can concatenate (merge) these files together into a single archieve by passing each file name to the command: 1 2 3 4 5 6 7 8 9 10 11 $ time tomahawk concat -i part1_3.two -i part2_3.two -i part3_3.two -o part3_concat [ 2019 -01-22 10 :55:47,799 ][ LOG ] All files are compatible. Beginning merging... [ 2019 -01-22 10 :55:47,800 ][ LOG ] Appending part1_3.two... 397 .953344 Mb/98.987848 Mb [ 2019 -01-22 10 :55:47,889 ][ LOG ] Appending part2_3.two... 359 .538884 Mb/54.763322 Mb [ 2019 -01-22 10 :55:47,938 ][ LOG ] Appending part3_3.two... 346 .075500 Mb/52.072741 Mb [ 2019 -01-22 10 :55:47,988 ][ LOG ] Finished. Added 3 files... [ 2019 -01-22 10 :55:47,988 ][ LOG ] Total size: Uncompressed = 1 .103568 Gb and compressed = 205 .823911 Mb real 0m0.226s user 0m0.036s sys 0m0.190s Passing every single input file in the command line does not become feasble when we have many hundreds to thousands of files. To address this, it is possible to first store the target file paths in a text file and then pass that to Tomahawk. Using the data from above, we save these file paths to the file part_file_list.txt . 1 for i in { 1 ..3 } ; do echo part $i \\_ 3 .two >> part_file_list.txt ; done Then we simply pass this list of file paths to Tomahawk using the -I argument: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ time tomahawk concat -I part_file_list.txt -o part3_concat adding file = part1_3.two adding file = part2_3.two adding file = part3_3.two [ 2019 -01-22 10 :57:43,227 ][ LOG ] All files are compatible. Beginning merging... [ 2019 -01-22 10 :57:43,262 ][ LOG ] Appending part1_3.two... 397 .953344 Mb/98.987848 Mb [ 2019 -01-22 10 :57:43,350 ][ LOG ] Appending part2_3.two... 359 .538884 Mb/54.763322 Mb [ 2019 -01-22 10 :57:43,400 ][ LOG ] Appending part3_3.two... 346 .075500 Mb/52.072741 Mb [ 2019 -01-22 10 :57:43,447 ][ LOG ] Finished. Added 3 files... [ 2019 -01-22 10 :57:43,447 ][ LOG ] Total size: Uncompressed = 1 .103568 Gb and compressed = 205 .823911 Mb real 0m0.477s user 0m0.040s sys 0m0.241s Possible duplications The concat subroutine will dedupe input file paths but will neiter check nor guarantee that the actual input data is not duplicated. Therefore, it is possible to get duplicates in your data if you are not careful. These duplicate entries could corrupt any downstream insights! Sorting output files \u00b6 All subroutines in Tomahawk will work on unsorted output files. However, sorted files are much faster to query and will result in considerable time savings. This is especially true for very large files. Sorting files in Tomahawk is trivial and involves calling the sort command with the required fields input ( -i ) and output ( -o ). Disk usage Tomahawk can easily sort huge .two files with the help of external merge sorting algorithms followed by a single k-way merge , memory assisted operation. The external merge step require additional disk space approximately equal to the size of the input dataset. Then, the merge operation will write a, generally, smaller output file. On average, we can approxiate that the sort operation require an additional two times the input file size on disk. After the sorting procedure is completed, the intermediate files are removed. Temporary files By default, Tomahawk will generate temporary files in the same directory as the input file. If this file path is undesired then you have to modify the appropriate parameters. In this example we will sort the almost 500 million (473,514,826) records computed from the sliding window example above. This archive corresponds to >50 Gb of binary data. 1 $ time tomahawk sort -i 1kgp3_chr6_4mb.two -o 1kgp3_chr6_4mb_sorted 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 Program: tomahawk-264d039a-dirty (Tools for computing, querying and storing LD data) Libraries: tomahawk-0.7.0; ZSTD-1.3.8; htslib 1.9 Contact: Marcus D. R. Klarqvist <mk819@cam.ac.uk> Documentation: https://github.com/mklarqvist/tomahawk License: MIT ---------- [2019-01-22 11:20:43,978][LOG] Calling sort... [2019-01-22 11:20:43,993][LOG] Blocks: 47,358 [2019-01-22 11:20:43,993][LOG] Uncompressed size: 50.192950 Gb [2019-01-22 11:20:43,993][LOG] Sorting 473,514,826 records... [2019-01-22 11:20:43,993][LOG][THREAD] Data/thread: 6.274119 Gb [2019-01-22 11:20:43,993][PROGRESS] Time elapsed Variants Progress Est. Time left [2019-01-22 11:20:43,994][LOG][THREAD] Slave-0: range=0->5919/47358 and name 1kgp3_chr6_4mb_sorted_fileSucRkC.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-1: range=5919->11838/47358 and name 1kgp3_chr6_4mb_sorted_filewPI10T.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-2: range=11838->17757/47358 and name 1kgp3_chr6_4mb_sorted_fileo1zcHb.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-3: range=17757->23676/47358 and name 1kgp3_chr6_4mb_sorted_fileBvRnnt.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-4: range=23676->29595/47358 and name 1kgp3_chr6_4mb_sorted_filewbBz3K.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-5: range=29595->35514/47358 and name 1kgp3_chr6_4mb_sorted_fileCrNLJ2.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-6: range=35514->41433/47358 and name 1kgp3_chr6_4mb_sorted_filerCfYpk.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-7: range=41433->47358/47358 and name 1kgp3_chr6_4mb_sorted_filexx9a6B.two [2019-01-22 11:21:13,994][PROGRESS] 30,000s 70,594,275 14.9086% 2m51s [2019-01-22 11:21:43,994][PROGRESS] 01m00,000s 146,732,835 30.988% 2m13s [2019-01-22 11:22:13,994][PROGRESS] 01m30,000s 245,535,675 51.8539% 1m23s [2019-01-22 11:22:43,995][PROGRESS] 02m00,001s 323,064,235 68.2268% 55s [2019-01-22 11:23:13,995][PROGRESS] 02m30,001s 421,592,790 89.0348% 18s [2019-01-22 11:23:24,951][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_filewbBz3K.two with 23676-29595. Sorted n=59190000 variants with size=1.176156 Gb [2019-01-22 11:23:25,114][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_fileBvRnnt.two with 17757-23676. Sorted n=59190000 variants with size=1.286938 Gb [2019-01-22 11:23:25,385][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_filerCfYpk.two with 35514-41433. Sorted n=59190000 variants with size=1.219126 Gb [2019-01-22 11:23:25,710][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_fileCrNLJ2.two with 29595-35514. Sorted n=59190000 variants with size=1.161081 Gb [2019-01-22 11:23:26,384][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_filexx9a6B.two with 41433-47358. Sorted n=59184826 variants with size=1.215231 Gb [2019-01-22 11:23:26,425][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_fileSucRkC.two with 0-5919. Sorted n=59190000 variants with size=1.242923 Gb [2019-01-22 11:23:29,966][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_fileo1zcHb.two with 11838-17757. Sorted n=59190000 variants with size=1.686758 Gb [2019-01-22 11:23:31,300][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_filewPI10T.two with 5919-11838. Sorted n=59190000 variants with size=2.013554 Gb [2019-01-22 11:23:31,338][PROGRESS] 02m47,344s 473,514,826 (2,829,587 variants/s) [2019-01-22 11:23:31,338][PROGRESS] Finished! [2019-01-22 11:23:31,341][LOG] Spawning 112 queues with 2.380952 Mb each... [2019-01-22 11:23:35,090][LOG][WRITER] Opening \"1kgp3_chr6_4mb_sorted.two\"... [2019-01-22 11:23:35,091][PROGRESS] Time elapsed Variants Progress Est. Time left [2019-01-22 11:24:05,091][PROGRESS] 30,000s 39,757,023 8.39615% 5m27s [2019-01-22 11:24:35,091][PROGRESS] 01m00,000s 76,990,000 16.2593% 5m9s [2019-01-22 11:25:05,091][PROGRESS] 01m30,000s 107,840,000 22.7744% 5m5s [2019-01-22 11:25:35,091][PROGRESS] 02m00,000s 133,420,000 28.1765% 5m5s [2019-01-22 11:26:05,092][PROGRESS] 02m30,000s 170,215,235 35.9472% 4m27s [2019-01-22 11:26:35,092][PROGRESS] 03m00,001s 208,070,000 43.9416% 3m49s [2019-01-22 11:27:05,092][PROGRESS] 03m30,001s 247,010,000 52.1652% 3m12s [2019-01-22 11:27:35,092][PROGRESS] 04m00,001s 287,690,000 60.7563% 2m35s [2019-01-22 11:28:05,092][PROGRESS] 04m30,001s 325,480,000 68.737% 2m2s [2019-01-22 11:28:35,093][PROGRESS] 05m00,001s 364,299,966 76.9353% 1m29s [2019-01-22 11:29:05,093][PROGRESS] 05m30,002s 401,367,427 84.7634% 59s [2019-01-22 11:29:35,093][PROGRESS] 06m00,002s 439,290,000 92.7722% 28s [2019-01-22 11:30:02,180][PROGRESS] 06m27,089s 473,514,826 (1,223,269 variants/s) [2019-01-22 11:30:02,180][PROGRESS] Finished! [2019-01-22 11:30:02,194][LOG] Finished merging! Time: 06m27,103s [2019-01-22 11:30:02,194][LOG] Deleting temp files... [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_fileSucRkC.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_filewPI10T.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_fileo1zcHb.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_fileBvRnnt.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_filewbBz3K.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_fileCrNLJ2.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_filerCfYpk.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_filexx9a6B.two [2019-01-22 11:30:02,678][LOG] Finished! real 9m18.793s user 23m31.390s sys 0m53.197s Faster queries Success: You can now use the newly created sorted archieve exactly as unsorted files but with faster query times. Format descriptions \u00b6 Memory layout (technical) A frequent design choice when designing a file format is how to align memory of records: either as lists of records (array-of-struct) versus column-orientated layouts (struct-of-array). The pivoted layout (struct-of-array) of twk1_two_t structs in Tomahawk will result considerable savings in disk space usage at the expense of querying speed of the resulting data. We decided to build the two format around the classical array-of-struct memory layout as we want to maximize the computability of the data. This is especially true in our application as we will never support individual columnar slicing and subset operations. LD format \u00b6 Tomahawk can output binary two data in the human-readable ld format by invoking the view command. The general schema for ld is below: Column Description FLAG Bit-packed boolean flags (see below) CHROM_A Chromosome for marker A POS_A Position for marker A CHROM_B Chromosome for marker B POS_B Position for marker B REF_REF Inner product of (0,0) haplotypes REF_ALT Inner product of (0,1) haplotypes ALT_REF Inner product of (1,0) haplotypes ALT_ALT Inner proruct of (1,1) haplotypes D Coefficient of linkage disequilibrium DPrime Normalized coefficient of linkage disequilibrium (scaled to [-1,1]) R Pearson correlation coefficient R2 Squared pearson correlation coefficient P Fisher's exact test P-value of the 2x2 haplotype contigency table ChiSqModel Chi-squared critical value of the 3x3 unphased table of the selected cubic root (\u03b1, \u03b2, or \u03b4) ChiSqTable Chi-squared critical value of table (useful comparator when P = 0) The 2x2 contingency table, or matrix, for the Fisher's exact test ( P ) for haplotypes look like this: REF-A REF-B REF-B A B ALT-B C D The 3x3 contigency table, or matrix, for the Chi-squared test for the unphased model looks like this: 0/0 0/1 1/1 0/0 A B C 0/1 D E F 1/1 G H J The two FLAG values are bit-packed booleans in a single integer field and describe a variety of states a pair of markers can be in. Bit position Numeric value One-hot Description 1 1 000000000000000 1 Used phased math. 2 2 00000000000000 1 0 Acceptor and donor variants are on the same contig. 3 4 0000000000000 1 00 Acceptor and donor variants are far apart on the same contig. 4 8 000000000000 1 000 The output contingency matrix has at least one empty cell (referred to as complete). 5 16 00000000000 1 0000 Output correlation coefficient is perfect (1.0). 6 32 0000000000 1 00000 Output solution is one of >1 possible solutions. This only occurs for unphased pairs. 7 64 000000000 1 000000 Output data was generated in 'fast mode'. 8 128 00000000 1 0000000 Output data is estimated from a subsampling of the total pool of genotypes. 9 256 0000000 1 00000000 Donor vector has missing value(s). 10 512 000000 1 000000000 Acceptor vector has missing value(s). 11 1024 00000 1 0000000000 Donor vector has low allele count (<5). 12 2048 0000 1 00000000000 Acceptor vector has low allele count (<5). 13 4096 000 1 000000000000 Acceptor vector has a HWE-P value < 1e-4. 14 8192 00 1 0000000000000 Donor vector has a HWE-P value < 1e-4. Viewing and manipulating output data \u00b6 It is possible to filter two output data by: 1) either start or end contig e.g. chr1 , 2) position in that contig e.g. chr1:10e6-20e6 ; 3) have a particular contig mapping e.g. chr1,chr2 ; 4) interval mapping in both contigs e.g. chr1:10e3-10e6,chr2:0-10e6 1 tomahawk view -i 1kgp3_chr6_1_45.two -I 6 :10e3-10e6,6:0-10e6 -H | head -n 6 Viewing all data 1 tomahawk view -i 1kgp3_chr6_1_45.two -H | head -n 6 FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D DPrime R R2 P ChiSqModel ChiSqTable 2059 6 89572 6 214654 4999 6 0 3 0.000597965 1 0.577004 0.332934 4.01511e-09 1667.33 0 2059 6 89573 6 214654 4999 6 0 3 0.000597965 1 0.577004 0.332934 4.01511e-09 1667.33 0 2059 6 122855 6 214654 4988 17 0 3 0.000596649 1 0.38664 0.149491 5.44908e-08 748.648 0 3 6 143500 6 212570 4421 387 82 118 0.0195352 0.54402 0.331324 0.109775 1.53587e-69 549.755 0 3 6 143500 6 213499 4419 387 84 118 0.0194949 0.537523 0.329068 0.108286 7.57426e-69 542.296 0 Slicing a range 1 tomahawk view -i 1kgp3_chr6_1_45.two -H -I 6 :5e6-6e6 | head -n 6 FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D DPrime R R2 P ChiSqModel ChiSqTable 2063 6 5022893 6 73938 5000 7 0 1 0.000199362 1 0.353306 0.124825 0.00159744 625.125 0 2063 6 5020564 6 89339 5000 7 0 1 0.000199362 1 0.353306 0.124825 0.00159744 625.125 0 7 6 5018459 6 156100 1150 1283 388 2187 0.0804323 0.509361 0.348864 0.121706 1.17649e-138 609.504 0 7 6 5018691 6 156100 861 811 677 2659 0.0693918 0.339199 0.31898 0.101748 1.10017e-109 509.554 0 7 6 5018910 6 156100 861 811 677 2659 0.0693918 0.339199 0.31898 0.101748 1.10017e-109 509.554 0 Slicing matches in B string 1 tomahawk view -i 1kgp3_chr6_1_45.two -H -I 6 :5e6-6e6,6:5e6-6e6 | head -n 6 FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D DPrime R R2 P ChiSqModel ChiSqTable 3 6 5000012 6 5073477 4988 9 5 6 0.0011915 0.544089 0.465743 0.216917 1.05037e-13 1086.32 0 1027 6 5000160 6 5072168 5000 2 4 2 0.000398404 0.4994 0.407677 0.166201 7.1708e-06 832.333 0 1027 6 5000160 6 5078340 5000 2 4 2 0.000398404 0.4994 0.407677 0.166201 7.1708e-06 832.333 0 3 6 5000482 6 5079621 4993 6 2 7 0.0013931 0.777199 0.64641 0.417846 3.9492e-18 2092.57 0 3 6 5000482 6 5082227 4994 6 1 7 0.00139362 0.874675 0.685808 0.470333 8.78523e-19 2355.43 0 As alluded to in the sorting section , sorted files have generally much faster query times. We can demonstrate this difference by using the sorted and unsorted data from the sliding window example above. 1 2 3 4 5 $ time tomahawk view -i 1kgp3_chr6_4mb.two -H -I 6 :1e6-5e6,6:1e6-5e6 > /dev/null real 3m1.029s user 2m41.929s sys 0m5.682s Sorted 1 2 3 4 5 $ time tomahawk view -i 1kgp3_chr6_4mb_sorted.two -H -I 6 :1e6-5e6,6:1e6-5e6 > /dev/null real 0m38.167s user 0m37.579s sys 0m0.192s Viewing ld data from the binary two file format and filtering out lines with a Fisher's exact test P-value < 1e-4, minor haplotype frequency < 5 and have both markers on the same contig (bit 2 ) 1 tomahawk view -i file.two -P 1e-4 -a 5 -f 2 Example output FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D Dprime R R2 P ChiSqModel ChiSqTable 15 20 1314874 20 2000219 5002 5 0 1 0.00019944127 1 0.4080444 0.16650023 0.0011980831 0 833.83313 15 20 1315271 20 1992301 5005 2 0 1 0.00019956089 1 0.57723492 0.33320019 0.00059904153 0 1668.6665 15 20 1315527 20 1991024 5004 0 3 1 0.00019952102 1 0.49985018 0.2498502 0.00079872204 0 1251.2498 15 20 1315763 20 1982489 5006 0 1 1 0.00019960076 1 0.70703614 0.49990013 0.00039936102 0 2503.4999 15 20 1315807 20 1982446 5004 3 0 1 0.00019952102 1 0.49985018 0.2498502 0.00079872204 0 1251.2498 Example unphased output. Notice that estimated haplotype counts are now floating values and the bit-flag 1 is not set. FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D Dprime R R2 P ChiSqModel ChiSqTable 46 20 1882564 20 1306588 5003.9996 2.0003999 1.0003999 0.99960008 0.00019936141 0.49950022 0.40779945 0.1663004 0.0011978438 0.0011996795 832.83241 46 20 1895185 20 1306588 5004.9998 1.0001999 1.0001999 0.99980011 0.0001994811 0.49970025 0.49970022 0.24970032 0.00079864228 0.00069960005 1250.4992 46 20 1306588 20 1901581 5003.9996 2.0003999 1.0003999 0.99960008 0.00019936141 0.49950022 0.40779945 0.1663004 0.0011978438 0.0011996795 832.83241 46 20 1901581 20 1306588 5003.9996 2.0003999 1.0003999 0.99960008 0.00019936141 0.49950022 0.40779945 0.1663004 0.0011978438 0.0011996795 832.83241 46 20 1306649 20 1885268 5006 1 1.2656777e-08 0.99999999 0.00019960076 1 0.70703614 0.49990013 0.00039936102 0.00039969285 2503.4999 Example output for forced phased math in fast mode. Note that only the REF_REF count is available, the fast math bit-flag is set, and all P and Chi-squared CV values are 0. FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D Dprime R R2 P ChiSqModel ChiSqTable 67 20 1345922 20 1363176 4928 0 0 0 0.011788686 0.98334378 0.86251211 0.74392718 0 0 0 67 20 1345922 20 1367160 4933 0 0 0 0.011800847 0.98336071 0.89164203 0.79502547 0 0 0 67 20 1345958 20 1348347 4944 0 0 0 0.0092644105 0.97890127 0.85316211 0.72788554 0 0 0 67 20 1345958 20 1354524 4938 0 0 0 0.0092493389 0.86871892 0.80354655 0.64568704 0 0 0 75 20 1345958 20 1356626 4945 0 0 0 0.0033518653 0.99999994 0.51706308 0.26735422 0 0 0 Aggregating and visualizing datasets \u00b6 Tomahawk generally output many millions to many hundreds of millions to billions of output linkage disequilibrium (LD) associations generated from many millions of input SNVs. It is technically very challenging to visualize such large datasets. Read more about aggregation in Tomahawk. Aggregation Description R Pearson correlation coefficient R2 Squared pearson correlation coefficient D Coefficient of linkage disequilibrium Dprime Scaled coefficient of linkage disequilibrium Dp Alias for dprime P Fisher's exact test P-value of the 2x2 haplotype contigency table Hets Number of (0,1) or (1,0) associations Alts Number of (1,1) associations Het Alias for hets Alt Alias for alts Reduction Description Mean Mean number of aggregate Max Largest number in aggregate bin Min Smallest number in aggregate bin Count Total number of records in a bin N Alias for count Total Sum total of aggregated number in a bin In this example, we will aggregate the genome-wide data generated from the sliding example above by aggregating on R2 values and reducing into count in a (4000,4000)-bin space. If a bin have less than 50 observations ( -c ) we will drop all the value and report 0 for that bin. 1 twk aggregate -i 1kgp3_chr6_4mb_sorted.two -x 4000 -y 4000 -f r2 -r count -c 50 -t 4 -O b -o 1kgp3_chr6_4mb_aggregate.twa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Program: tomahawk-7f8eef9b-dirty (Tools for computing, querying and storing LD data) Libraries: tomahawk-0.7.0; ZSTD-1.3.8; htslib 1.9 Contact: Marcus D. R. Klarqvist <mk819@cam.ac.uk> Documentation: https://github.com/mklarqvist/tomahawk License: MIT ---------- [2019-01-25 14:55:43,299][LOG] Calling aggregate... [2019-01-25 14:55:43,299][LOG] Performing 2-pass over data... [2019-01-25 14:55:43,299][LOG] ===== First pass (peeking at landscape) ===== [2019-01-25 14:55:43,299][LOG] Blocks: 47,352 [2019-01-25 14:55:43,299][LOG] Uncompressed size: 50.192950 Gb [2019-01-25 14:55:43,299][LOG][THREAD] Data/thread: 12.548238 Gb [2019-01-25 14:55:43,299][PROGRESS] Time elapsed Variants Progress Est. Time left [2019-01-25 14:56:13,299][PROGRESS] 30,000s 344,930,000 72.8446% 11s [2019-01-25 14:56:27,979][PROGRESS] 44,679s 473,514,826 (10,597,935 variants/s) [2019-01-25 14:56:27,979][PROGRESS] Finished! [2019-01-25 14:56:27,979][LOG] ===== Second pass (building matrix) ===== [2019-01-25 14:56:27,979][LOG] Aggregating 473,514,826 records... [2019-01-25 14:56:27,979][LOG][THREAD] Allocating: 2.560000 Gb for matrices... [2019-01-25 14:56:27,979][PROGRESS] Time elapsed Variants Progress Est. Time left [2019-01-25 14:56:57,979][PROGRESS] 30,000s 348,560,000 73.6112% 10s [2019-01-25 14:57:12,499][PROGRESS] 44,520s 473,514,826 (10,635,926 variants/s) [2019-01-25 14:57:12,499][PROGRESS] Finished! [2019-01-25 14:57:13,713][LOG] Aggregated 473,514,826 records in 16,000,000 bins. [2019-01-25 14:57:13,713][LOG] Finished. You can now use the output binary format .twa in your downstream analysis. Alternatively, it is possible to output a human-readable (x,y)-matrix by setting the -O parameter to u . This tab-delimited matrix can now be loaded in any programming language and used as input for graphical visualizations or for analysis. It is easiest to use these files directly in rtomahawk , the R-bindings for tomahawk .","title":"CLI"},{"location":"tutorial/#getting-started-with-tomahawk","text":"","title":"Getting started with tomahawk"},{"location":"tutorial/#about","text":"This is an introductory tutorial for using Tomahawk. It will cover: Importing data into Tomahawk Computing linkage-disequilibrium Subsetting and filtering output data Aggregating datasets","title":"About"},{"location":"tutorial/#usage-instructions","text":"The CLI of Tomahawk comprises of several distinct subroutines (listed below). Executing tomahawk gives a list of commands with brief descriptions and tomahawk <command> gives detailed details for that command. All primary Tomahawk commands operate on the binary Tomahawk twk and Tomahawk output two file format. Interconversions between twk and vcf / bcf is supported through the commands import for vcf / bcf -> twk and view for twk -> vcf . Linkage disequilibrium data is written out in binary two format or human-readable ld format. Command Description aggregate data rasterization framework for TWO files calc calculate linkage disequilibrium scalc calculate linkage disequilibrium for a single site concat concatenate TWO files from the same set of samples import import VCF / VCF.gz / BCF to TWK sort sort TWO file view TWO -> LD / TWO view, TWO subset and filter haplotype extract per-sample haplotype strings in FASTA /binary format relationship compute marker-based pair-wise sample relationship matrices decay compute LD-decay over distance prune perform graph-based LD-pruning of variant sites","title":"Usage instructions"},{"location":"tutorial/#importing-into-tomahawk","text":"By design Tomahawk only operates on diploid and bi-allelic SNVs and as such filters out indels and complex variants. Tomahawk does not support mixed phasing of genotypes in the same variant (e.g. 0|0 , 0/1 ). If mixed phasing is found for a record, all genotypes for that site are converted to unphased genotypes. This is a conscious design choice as this will internally invoke the correct algorithm to use for mixed-phase cases. Importing standard files to Tomahawk involes using the import command. The following command imports a bcf file and outputs file.twk while filtering out variants with >20% missingness and sites that deviate from Hardy-Weinberg equilibrium with a probability < 0.001. 1 tomahawk import -i file.bcf -o file -m 0 .2 -H 1e-3 1 2 $ md5sum 1kgp3_chr6.bcf 8c05554ebe1a51e99be2471c96fad4d9 1kgp3_chr6.bcf 1 2 $ bcftools view 1kgp3_chr6.bcf -HG | wc -l 5024119 1 $ time tomahawk import -i 1kgp3_chr6.bcf -o 1kgp3_chr6 Auto-completion of file extensions You do not have to append the .twk suffix to the output name as Tomahawk will automatically add this if missing. In the example above \"1kgp3_chr6\" will be converted to \"1kgp3_chr6.twk\" automatically. This is true for most Tomahawk commands when using the CLI. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Program: tomahawk-264d039a-dirty (Tools for computing, querying and storing LD data) Libraries: tomahawk-0.7.0; ZSTD-1.3.8; htslib 1.9 Contact: Marcus D. R. Klarqvist <mk819@cam.ac.uk> Documentation: https://github.com/mklarqvist/tomahawk License: MIT ---------- [2019-01-21 11:58:15,692][LOG] Calling import... [2019-01-21 11:58:15,692][LOG][READER] Opening 1kgp3_chr6.bcf... [2019-01-21 11:58:15,695][LOG][VCF] Constructing lookup table for 86 contigs... [2019-01-21 11:58:15,695][LOG][VCF] Samples: 2,504... [2019-01-21 11:58:15,695][LOG][WRITER] Opening 1kgp3_chr6.twk... [2019-01-21 11:58:39,307][LOG] Duplicate site dropped: 6:18233985 [2019-01-21 11:59:26,749][LOG] Duplicate site dropped: 6:55137646 [2019-01-21 11:59:39,315][LOG] Duplicate site dropped: 6:67839893 [2019-01-21 11:59:47,176][LOG] Duplicate site dropped: 6:74373442 [2019-01-21 11:59:51,440][LOG] Duplicate site dropped: 6:77843171 [2019-01-21 12:00:41,784][LOG] Duplicate site dropped: 6:121316830 [2019-01-21 12:01:12,830][LOG] Duplicate site dropped: 6:148573620 [2019-01-21 12:01:16,557][LOG] Duplicate site dropped: 6:151397786 [2019-01-21 12:01:42,644][LOG] Wrote: 4,784,608 variants to 9,570 blocks... [2019-01-21 12:01:42,644][LOG] Finished: 03m26,952s [2019-01-21 12:01:42,644][LOG] Filtered out 239,511 sites (4.76722%): [2019-01-21 12:01:42,645][LOG] Invariant: 15,485 (0.308213%) [2019-01-21 12:01:42,645][LOG] Missing threshold: 0 (0%) [2019-01-21 12:01:42,645][LOG] Insufficient samples: 0 (0%) [2019-01-21 12:01:42,645][LOG] Mixed ploidy: 0 (0%) [2019-01-21 12:01:42,645][LOG] No genotypes: 0 (0%) [2019-01-21 12:01:42,645][LOG] No FORMAT: 0 (0%) [2019-01-21 12:01:42,645][LOG] Not biallelic: 26,277 (0.523017%) [2019-01-21 12:01:42,645][LOG] Not SNP: 194,566 (3.87264%)","title":"Importing into Tomahawk"},{"location":"tutorial/#computing-linkage-disequilibrium","text":"","title":"Computing linkage-disequilibrium"},{"location":"tutorial/#all-pairwise-comparisons","text":"In this first example, we will compute all-vs-all LD associations using the data we imported in the previous section. To limit compute, we restrict our attention to a 1/45 section of the data by passing the -c and -C job parameters. We will be using 8 threads ( -t ), but you may need to modify this to match the hardware available on your host machine. This job involves comparing a pair of variants >141 billion times and as such takes around 30 min to finish on most machines. 1 $ tomahawk calc -pi 1kgp3_chr6.twk -o 1kgp3_chr6_1_45 -C 1 -c 45 -t 8 Valid job balancing partitions When computing genome-wide LD the balancing requires that number of sub-problems ( -c ) must be a member of the function: $$ \\binom{c}{2} + c = \\frac{c^2 + c}{2}, c > 0 $$ This function is equivalent to the upper-triangular of a square ( c -by- c ) matrix plus the diagonal. Read more about load partitioning in Tomahawk. Here is the first 100 valid partition sizes: 1 , 3 , 6 , 10 , 15 , 21 , 28 , 36 , 45 , 55 , 66 , 78 , 91 , 105 , 120 , 136 , 153 , 171 , 190 , 210 , 231 , 253 , 276 , 300 , 325 , 351 , 378 , 406 , 435 , 465 , 496 , 528 , 561 , 595 , 630 , 666 , 703 , 741 , 780 , 820 , 861 , 903 , 946 , 990 , 1035 , 1081 , 1128 , 1176 , 1225 , 1275 , 1326 , 1378 , 1431 , 1485 , 1540 , 1596 , 1653 , 1711 , 1770 , 1830 , 1891 , 1953 , 2016 , 2080 , 2145 , 2211 , 2278 , 2346 , 2415 , 2485 , 2556 , 2628 , 2701 , 2775 , 2850 , 2926 , 3003 , 3081 , 3160 , 3240 , 3321 , 3403 , 3486 , 3570 , 3655 , 3741 , 3828 , 3916 , 4005 , 4095 , 4186 , 4278 , 4371 , 4465 , 4560 , 4656 , 4753 , 4851 , 4950 , 5050 Massive workload By default, Tomahawk will attempt to compute the genome-wide linkage-disequilibrium for all the provided variants and samples. This generally require huge amount of pairwise variant comparisons. Depending on your interests, you may not want to undertake this large compute and need to parameterize for your particular use-case. Different load balancing As a consequence of the job balancing approach in Tomahawk, the diagonal jobs will always involve less variant comparisons (\\( \\binom{n}{2} \\)) compared to off-diagonal jobs (\\(n \\times m\\)), where \\(n\\) and \\(m\\) are the number of records in either block. Keep this in mind if you are choosing a partition size given the run-times of a given job. For example, examine the total run-time of the second job ( -C 2 ) \u2014 the first off-diagonal job \u2014 as a proxy for the expected average run-time per job. Output ordering By design, Tomahawk computes pairwise associations using an out-of-order execution paradigm and further permutes the input ordering by using cache-blocking . Additionally, each thread/slave transiently store a number of computed associations in a private buffer that will be flushed upon reaching some frequency threshold. Because of these technicalities, Tomahawk will neither produce ordered output nor will the permutation of the output order be identical between runs on identical data. This has no practical consequence to most downstream applications in Tomahawk with the exception of query speeds. To address this limitation we have introduced a powerful sorting paradigm that will be discussed below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Program: tomahawk-264d039a-dirty (Tools for computing, querying and storing LD data) Libraries: tomahawk-0.7.0; ZSTD-1.3.8; htslib 1.9 Contact: Marcus D. R. Klarqvist <mk819@cam.ac.uk> Documentation: https://github.com/mklarqvist/tomahawk License: MIT ---------- [2019-01-21 12:13:50,210][LOG] Calling calc... [2019-01-21 12:13:50,211][LOG][READER] Opening 1kgp3_chr6.twk... [2019-01-21 12:13:50,212][LOG] Samples: 2,504... [2019-01-21 12:13:50,212][LOG][BALANCING] Using ranges [0-1063,0-1063] in square mode... [2019-01-21 12:13:50,212][LOG] Allocating 1,063 blocks... [2019-01-21 12:13:50,213][LOG] Running in standard mode. Pre-computing data... [2019-01-21 12:13:50,213][LOG][SIMD] Vectorized instructions available: SSE4... [2019-01-21 12:13:50,213][LOG] Constructing list, vector, RLE... [2019-01-21 12:13:50,213][LOG][THREAD] Unpacking using 7 threads: ....... Done! 01,291s [2019-01-21 12:13:51,504][LOG] 531,500 variants from 1,063 blocks... [2019-01-21 12:13:51,504][LOG][PARAMS] square=TRUE,window=FALSE,low_memory=FALSE,bitmaps=FALSE,single=FALSE,force_phased=TRUE,force_unphased=FALSE,compression_level=1,block_size=500,output_block_size=10000,l_surrounding=500000,minP=1.000000,minR2=0.100000,maxR2=100.000000,minDprime=0.000000,maxDprime=100.000000,n_chunks=45,c_chunk=0,n_threads=8,ldd_type=3,cycle_threshold=0 [2019-01-21 12:13:51,504][LOG] Performing: 141,245,859,250 variant comparisons... [2019-01-21 12:13:51,504][LOG][WRITER] Opening 1kgp3_chr6_1_45.two... [2019-01-21 12:13:51,505][LOG][THREAD] Spawning 8 threads: ........ [2019-01-21 12:13:51,533][PROGRESS] Time elapsed Variants Genotypes Output Progress Est. Time left [2019-01-21 12:14:21,533][PROGRESS] 30,000s 3,685,746,500 9,229,109,236,000 988,858 2.60945% 18m39s [2019-01-21 12:14:51,533][PROGRESS] 01m00,000s 6,425,868,750 16,090,375,350,000 1,689,422 4.54942% 20m58s [truncated] [2019-01-21 12:39:51,546][PROGRESS] 26m00,013s 140,131,382,750 350,888,982,406,000 48,635,114 99.211% 12s [2019-01-21 12:40:04,317][PROGRESS] Finished in 26m12,784s. Variants: 141,245,859,250, genotypes: 353,679,631,562,000, output: 49,870,388 [2019-01-21 12:40:04,317][PROGRESS] 89,806,242 variants/s and 224,874,830,855 genotypes/s [2019-01-21 12:40:04,321][LOG][PROGRESS] All done...26m12,815s! This run generated 935817820 bytes (935.8 MB) of output data in binary format using the default compression level (1): 1 2 $ ls -l 1kgp3_chr6_1_45.two -rw-rw-r-- 1 mk819 mk819 935817820 Jan 21 12 :40 1kgp3_chr6_1_45.two If this data was stored in plain, human-readable, text format it would use over 45 GB: 1 2 $ tomahawk view -i 1kgp3_chr6_1_45.two | wc -c 4544702179","title":"All pairwise comparisons"},{"location":"tutorial/#sliding-window","text":"If you are working with a species with well-know LD structure (such as humans) you can reduce the computational cost by limiting the search-space to a fixed-sized sliding window ( -w ). In window mode you are free to choose any arbitrary sub-problem ( -c ) size. Window properties By default, Tomahawk computes the linkage disequilibrium associations for all pairs of variants within -w bases of the target SNV on either direction. In order to maximize computational throughput, we made the design decision to maintain blocks of data that overlap to the target interval. This has the consequence that adjacent data that may not directly overlap with the target region will still be computed. The technical reasoning for this is, in simplified terms, that the online repackaging of internal data blocks is more expensive than computing a small (relatively) number of non-desired (off-target) associations. 1 time tomahawk calc -pi 1kgp3_chr6.twk -o 1kgp3_chr6_4mb -w 4000000 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Program: tomahawk-264d039a-dirty (Tools for computing, querying and storing LD data) Libraries: tomahawk-0.7.0; ZSTD-1.3.8; htslib 1.9 Contact: Marcus D. R. Klarqvist <mk819@cam.ac.uk> Documentation: https://github.com/mklarqvist/tomahawk License: MIT ---------- [2019-01-21 14:10:33,616][LOG] Calling calc... [2019-01-21 14:10:33,616][LOG][READER] Opening 1kgp3_chr6.twk... [2019-01-21 14:10:33,619][LOG] Samples: 2,504... [2019-01-21 14:10:33,619][LOG][BALANCING] Using ranges [0-9570,0-9570] in window mode... [2019-01-21 14:10:33,619][LOG] Allocating 9,570 blocks... [2019-01-21 14:10:33,620][LOG] Running in standard mode. Pre-computing data... [2019-01-21 14:10:33,620][LOG][SIMD] Vectorized instructions available: SSE4... [2019-01-21 14:10:33,620][LOG] Constructing list, vector, RLE... [2019-01-21 14:10:33,620][LOG][THREAD] Unpacking using 7 threads: ....... Done! 15,774s [2019-01-21 14:10:49,394][LOG] 4,784,608 variants from 9,570 blocks... [2019-01-21 14:10:49,394][LOG][PARAMS] square=TRUE,window=TRUE,low_memory=FALSE,bitmaps=FALSE,single=FALSE,force_phased=TRUE,force_unphased=FALSE,compression_level=1,block_size=500,output_block_size=10000,window_size=4000000,l_surrounding=500000,minP=1.000000,minR2=0.100000,maxR2=100.000000,minDprime=0.000000,maxDprime=100.000000,n_chunks=1,c_chunk=0,n_threads=8,ldd_type=3,cycle_threshold=0 [2019-01-21 14:10:49,394][LOG] Performing: 11,446,234,464,528 variant comparisons... [2019-01-21 14:10:49,394][LOG][WRITER] Opening 1kgp3_chr6_4mb.two... [2019-01-21 14:10:49,402][LOG][THREAD] Spawning 8 threads: ........ [2019-01-21 14:10:49,424][PROGRESS] Time elapsed Variants Genotypes Output Progress Est. Time left [2019-01-21 14:11:19,424][PROGRESS] 30,000s 2,311,240,500 5,787,346,212,000 853,368 0 0 [2019-01-21 14:11:49,424][PROGRESS] 01m00,000s 4,562,731,500 11,425,079,676,000 1,717,956 0 0 [truncated] [2019-01-21 16:12:49,492][PROGRESS] 02h02m00,068s 527,889,726,500 1,321,835,875,156,000 470,354,846 0 0 [2019-01-21 16:13:17,236][PROGRESS] Finished in 02h02m27,812s. Variants: 529,807,522,528, genotypes: 1,326,638,036,410,112, output: 473,514,018 [2019-01-21 16:13:17,237][PROGRESS] 72,104,114 variants/s and 180,548,701,880 genotypes/s [2019-01-21 16:13:17,257][LOG][PROGRESS] All done...02h02m27,854s!","title":"Sliding window"},{"location":"tutorial/#single-interval-vs-its-local-neighbourhood","text":"In many cases we are not interested in computing large-scale linkage-disequilibrium associations but have a limited genomics window of interest. If this is the case, you can use the specialization subroutine of calc called scalc that is parameterizable with a target region and a neighbourhood size in bases. In this example, we will compute the regional associations of variants mapping to the interval chr6:10e6-10.1e6 and a 100kb neighbourhood. 1 tomahawk scalc -i 1kgp3_chr6.twk -o test -I 6 :10e6-10.1e6 -w 100000 Neighbourhood interval The neighbourhood is computed as the region from [start of interval - neighbourhood, start of interval) and (end of interval + neighbourhood). If the start of the interval minus the neighbourhood falls outside the chromosome (position < 0) then this interval is truncated to 0 in the left end. Scalability This subroutine was designed to be used with a relatively short interval to maximize computability in this case. It is however possible to provide any valid interval as the target region to compute linkage-disequilibrium for. Providing large intervals will result in poor performance and potentially undesired output.","title":"Single interval vs its local neighbourhood"},{"location":"tutorial/#concatenating-multiple-archives","text":"On of the immediate downsides of partitioning compute into multiple non-overlapping sub-problems is that we will generate a large number of independent files that must be merged prior to downstream analysis. Fortunatly, this is a trivial operation in Tomahawk and involves the concat command. First lets compute some data using the first 3 out of 990 partitions of the dataset used above. 1 for i in { 1 ..3 } ; do time tomahawk calc -pi 1kgp3_chr6.twk -c 990 -C $i -o part $i \\_ 3 .two ; done Next, since we only have three files, we can concatenate (merge) these files together into a single archieve by passing each file name to the command: 1 2 3 4 5 6 7 8 9 10 11 $ time tomahawk concat -i part1_3.two -i part2_3.two -i part3_3.two -o part3_concat [ 2019 -01-22 10 :55:47,799 ][ LOG ] All files are compatible. Beginning merging... [ 2019 -01-22 10 :55:47,800 ][ LOG ] Appending part1_3.two... 397 .953344 Mb/98.987848 Mb [ 2019 -01-22 10 :55:47,889 ][ LOG ] Appending part2_3.two... 359 .538884 Mb/54.763322 Mb [ 2019 -01-22 10 :55:47,938 ][ LOG ] Appending part3_3.two... 346 .075500 Mb/52.072741 Mb [ 2019 -01-22 10 :55:47,988 ][ LOG ] Finished. Added 3 files... [ 2019 -01-22 10 :55:47,988 ][ LOG ] Total size: Uncompressed = 1 .103568 Gb and compressed = 205 .823911 Mb real 0m0.226s user 0m0.036s sys 0m0.190s Passing every single input file in the command line does not become feasble when we have many hundreds to thousands of files. To address this, it is possible to first store the target file paths in a text file and then pass that to Tomahawk. Using the data from above, we save these file paths to the file part_file_list.txt . 1 for i in { 1 ..3 } ; do echo part $i \\_ 3 .two >> part_file_list.txt ; done Then we simply pass this list of file paths to Tomahawk using the -I argument: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ time tomahawk concat -I part_file_list.txt -o part3_concat adding file = part1_3.two adding file = part2_3.two adding file = part3_3.two [ 2019 -01-22 10 :57:43,227 ][ LOG ] All files are compatible. Beginning merging... [ 2019 -01-22 10 :57:43,262 ][ LOG ] Appending part1_3.two... 397 .953344 Mb/98.987848 Mb [ 2019 -01-22 10 :57:43,350 ][ LOG ] Appending part2_3.two... 359 .538884 Mb/54.763322 Mb [ 2019 -01-22 10 :57:43,400 ][ LOG ] Appending part3_3.two... 346 .075500 Mb/52.072741 Mb [ 2019 -01-22 10 :57:43,447 ][ LOG ] Finished. Added 3 files... [ 2019 -01-22 10 :57:43,447 ][ LOG ] Total size: Uncompressed = 1 .103568 Gb and compressed = 205 .823911 Mb real 0m0.477s user 0m0.040s sys 0m0.241s Possible duplications The concat subroutine will dedupe input file paths but will neiter check nor guarantee that the actual input data is not duplicated. Therefore, it is possible to get duplicates in your data if you are not careful. These duplicate entries could corrupt any downstream insights!","title":"Concatenating multiple archives"},{"location":"tutorial/#sorting-output-files","text":"All subroutines in Tomahawk will work on unsorted output files. However, sorted files are much faster to query and will result in considerable time savings. This is especially true for very large files. Sorting files in Tomahawk is trivial and involves calling the sort command with the required fields input ( -i ) and output ( -o ). Disk usage Tomahawk can easily sort huge .two files with the help of external merge sorting algorithms followed by a single k-way merge , memory assisted operation. The external merge step require additional disk space approximately equal to the size of the input dataset. Then, the merge operation will write a, generally, smaller output file. On average, we can approxiate that the sort operation require an additional two times the input file size on disk. After the sorting procedure is completed, the intermediate files are removed. Temporary files By default, Tomahawk will generate temporary files in the same directory as the input file. If this file path is undesired then you have to modify the appropriate parameters. In this example we will sort the almost 500 million (473,514,826) records computed from the sliding window example above. This archive corresponds to >50 Gb of binary data. 1 $ time tomahawk sort -i 1kgp3_chr6_4mb.two -o 1kgp3_chr6_4mb_sorted 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 Program: tomahawk-264d039a-dirty (Tools for computing, querying and storing LD data) Libraries: tomahawk-0.7.0; ZSTD-1.3.8; htslib 1.9 Contact: Marcus D. R. Klarqvist <mk819@cam.ac.uk> Documentation: https://github.com/mklarqvist/tomahawk License: MIT ---------- [2019-01-22 11:20:43,978][LOG] Calling sort... [2019-01-22 11:20:43,993][LOG] Blocks: 47,358 [2019-01-22 11:20:43,993][LOG] Uncompressed size: 50.192950 Gb [2019-01-22 11:20:43,993][LOG] Sorting 473,514,826 records... [2019-01-22 11:20:43,993][LOG][THREAD] Data/thread: 6.274119 Gb [2019-01-22 11:20:43,993][PROGRESS] Time elapsed Variants Progress Est. Time left [2019-01-22 11:20:43,994][LOG][THREAD] Slave-0: range=0->5919/47358 and name 1kgp3_chr6_4mb_sorted_fileSucRkC.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-1: range=5919->11838/47358 and name 1kgp3_chr6_4mb_sorted_filewPI10T.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-2: range=11838->17757/47358 and name 1kgp3_chr6_4mb_sorted_fileo1zcHb.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-3: range=17757->23676/47358 and name 1kgp3_chr6_4mb_sorted_fileBvRnnt.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-4: range=23676->29595/47358 and name 1kgp3_chr6_4mb_sorted_filewbBz3K.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-5: range=29595->35514/47358 and name 1kgp3_chr6_4mb_sorted_fileCrNLJ2.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-6: range=35514->41433/47358 and name 1kgp3_chr6_4mb_sorted_filerCfYpk.two [2019-01-22 11:20:43,994][LOG][THREAD] Slave-7: range=41433->47358/47358 and name 1kgp3_chr6_4mb_sorted_filexx9a6B.two [2019-01-22 11:21:13,994][PROGRESS] 30,000s 70,594,275 14.9086% 2m51s [2019-01-22 11:21:43,994][PROGRESS] 01m00,000s 146,732,835 30.988% 2m13s [2019-01-22 11:22:13,994][PROGRESS] 01m30,000s 245,535,675 51.8539% 1m23s [2019-01-22 11:22:43,995][PROGRESS] 02m00,001s 323,064,235 68.2268% 55s [2019-01-22 11:23:13,995][PROGRESS] 02m30,001s 421,592,790 89.0348% 18s [2019-01-22 11:23:24,951][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_filewbBz3K.two with 23676-29595. Sorted n=59190000 variants with size=1.176156 Gb [2019-01-22 11:23:25,114][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_fileBvRnnt.two with 17757-23676. Sorted n=59190000 variants with size=1.286938 Gb [2019-01-22 11:23:25,385][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_filerCfYpk.two with 35514-41433. Sorted n=59190000 variants with size=1.219126 Gb [2019-01-22 11:23:25,710][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_fileCrNLJ2.two with 29595-35514. Sorted n=59190000 variants with size=1.161081 Gb [2019-01-22 11:23:26,384][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_filexx9a6B.two with 41433-47358. Sorted n=59184826 variants with size=1.215231 Gb [2019-01-22 11:23:26,425][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_fileSucRkC.two with 0-5919. Sorted n=59190000 variants with size=1.242923 Gb [2019-01-22 11:23:29,966][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_fileo1zcHb.two with 11838-17757. Sorted n=59190000 variants with size=1.686758 Gb [2019-01-22 11:23:31,300][LOG][THREAD] Finished: 1kgp3_chr6_4mb_sorted_filewPI10T.two with 5919-11838. Sorted n=59190000 variants with size=2.013554 Gb [2019-01-22 11:23:31,338][PROGRESS] 02m47,344s 473,514,826 (2,829,587 variants/s) [2019-01-22 11:23:31,338][PROGRESS] Finished! [2019-01-22 11:23:31,341][LOG] Spawning 112 queues with 2.380952 Mb each... [2019-01-22 11:23:35,090][LOG][WRITER] Opening \"1kgp3_chr6_4mb_sorted.two\"... [2019-01-22 11:23:35,091][PROGRESS] Time elapsed Variants Progress Est. Time left [2019-01-22 11:24:05,091][PROGRESS] 30,000s 39,757,023 8.39615% 5m27s [2019-01-22 11:24:35,091][PROGRESS] 01m00,000s 76,990,000 16.2593% 5m9s [2019-01-22 11:25:05,091][PROGRESS] 01m30,000s 107,840,000 22.7744% 5m5s [2019-01-22 11:25:35,091][PROGRESS] 02m00,000s 133,420,000 28.1765% 5m5s [2019-01-22 11:26:05,092][PROGRESS] 02m30,000s 170,215,235 35.9472% 4m27s [2019-01-22 11:26:35,092][PROGRESS] 03m00,001s 208,070,000 43.9416% 3m49s [2019-01-22 11:27:05,092][PROGRESS] 03m30,001s 247,010,000 52.1652% 3m12s [2019-01-22 11:27:35,092][PROGRESS] 04m00,001s 287,690,000 60.7563% 2m35s [2019-01-22 11:28:05,092][PROGRESS] 04m30,001s 325,480,000 68.737% 2m2s [2019-01-22 11:28:35,093][PROGRESS] 05m00,001s 364,299,966 76.9353% 1m29s [2019-01-22 11:29:05,093][PROGRESS] 05m30,002s 401,367,427 84.7634% 59s [2019-01-22 11:29:35,093][PROGRESS] 06m00,002s 439,290,000 92.7722% 28s [2019-01-22 11:30:02,180][PROGRESS] 06m27,089s 473,514,826 (1,223,269 variants/s) [2019-01-22 11:30:02,180][PROGRESS] Finished! [2019-01-22 11:30:02,194][LOG] Finished merging! Time: 06m27,103s [2019-01-22 11:30:02,194][LOG] Deleting temp files... [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_fileSucRkC.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_filewPI10T.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_fileo1zcHb.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_fileBvRnnt.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_filewbBz3K.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_fileCrNLJ2.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_filerCfYpk.two [2019-01-22 11:30:02,194][LOG] Deleted 1kgp3_chr6_4mb_sorted_filexx9a6B.two [2019-01-22 11:30:02,678][LOG] Finished! real 9m18.793s user 23m31.390s sys 0m53.197s Faster queries Success: You can now use the newly created sorted archieve exactly as unsorted files but with faster query times.","title":"Sorting output files"},{"location":"tutorial/#format-descriptions","text":"Memory layout (technical) A frequent design choice when designing a file format is how to align memory of records: either as lists of records (array-of-struct) versus column-orientated layouts (struct-of-array). The pivoted layout (struct-of-array) of twk1_two_t structs in Tomahawk will result considerable savings in disk space usage at the expense of querying speed of the resulting data. We decided to build the two format around the classical array-of-struct memory layout as we want to maximize the computability of the data. This is especially true in our application as we will never support individual columnar slicing and subset operations.","title":"Format descriptions"},{"location":"tutorial/#ld-format","text":"Tomahawk can output binary two data in the human-readable ld format by invoking the view command. The general schema for ld is below: Column Description FLAG Bit-packed boolean flags (see below) CHROM_A Chromosome for marker A POS_A Position for marker A CHROM_B Chromosome for marker B POS_B Position for marker B REF_REF Inner product of (0,0) haplotypes REF_ALT Inner product of (0,1) haplotypes ALT_REF Inner product of (1,0) haplotypes ALT_ALT Inner proruct of (1,1) haplotypes D Coefficient of linkage disequilibrium DPrime Normalized coefficient of linkage disequilibrium (scaled to [-1,1]) R Pearson correlation coefficient R2 Squared pearson correlation coefficient P Fisher's exact test P-value of the 2x2 haplotype contigency table ChiSqModel Chi-squared critical value of the 3x3 unphased table of the selected cubic root (\u03b1, \u03b2, or \u03b4) ChiSqTable Chi-squared critical value of table (useful comparator when P = 0) The 2x2 contingency table, or matrix, for the Fisher's exact test ( P ) for haplotypes look like this: REF-A REF-B REF-B A B ALT-B C D The 3x3 contigency table, or matrix, for the Chi-squared test for the unphased model looks like this: 0/0 0/1 1/1 0/0 A B C 0/1 D E F 1/1 G H J The two FLAG values are bit-packed booleans in a single integer field and describe a variety of states a pair of markers can be in. Bit position Numeric value One-hot Description 1 1 000000000000000 1 Used phased math. 2 2 00000000000000 1 0 Acceptor and donor variants are on the same contig. 3 4 0000000000000 1 00 Acceptor and donor variants are far apart on the same contig. 4 8 000000000000 1 000 The output contingency matrix has at least one empty cell (referred to as complete). 5 16 00000000000 1 0000 Output correlation coefficient is perfect (1.0). 6 32 0000000000 1 00000 Output solution is one of >1 possible solutions. This only occurs for unphased pairs. 7 64 000000000 1 000000 Output data was generated in 'fast mode'. 8 128 00000000 1 0000000 Output data is estimated from a subsampling of the total pool of genotypes. 9 256 0000000 1 00000000 Donor vector has missing value(s). 10 512 000000 1 000000000 Acceptor vector has missing value(s). 11 1024 00000 1 0000000000 Donor vector has low allele count (<5). 12 2048 0000 1 00000000000 Acceptor vector has low allele count (<5). 13 4096 000 1 000000000000 Acceptor vector has a HWE-P value < 1e-4. 14 8192 00 1 0000000000000 Donor vector has a HWE-P value < 1e-4.","title":"LD format"},{"location":"tutorial/#viewing-and-manipulating-output-data","text":"It is possible to filter two output data by: 1) either start or end contig e.g. chr1 , 2) position in that contig e.g. chr1:10e6-20e6 ; 3) have a particular contig mapping e.g. chr1,chr2 ; 4) interval mapping in both contigs e.g. chr1:10e3-10e6,chr2:0-10e6 1 tomahawk view -i 1kgp3_chr6_1_45.two -I 6 :10e3-10e6,6:0-10e6 -H | head -n 6 Viewing all data 1 tomahawk view -i 1kgp3_chr6_1_45.two -H | head -n 6 FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D DPrime R R2 P ChiSqModel ChiSqTable 2059 6 89572 6 214654 4999 6 0 3 0.000597965 1 0.577004 0.332934 4.01511e-09 1667.33 0 2059 6 89573 6 214654 4999 6 0 3 0.000597965 1 0.577004 0.332934 4.01511e-09 1667.33 0 2059 6 122855 6 214654 4988 17 0 3 0.000596649 1 0.38664 0.149491 5.44908e-08 748.648 0 3 6 143500 6 212570 4421 387 82 118 0.0195352 0.54402 0.331324 0.109775 1.53587e-69 549.755 0 3 6 143500 6 213499 4419 387 84 118 0.0194949 0.537523 0.329068 0.108286 7.57426e-69 542.296 0 Slicing a range 1 tomahawk view -i 1kgp3_chr6_1_45.two -H -I 6 :5e6-6e6 | head -n 6 FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D DPrime R R2 P ChiSqModel ChiSqTable 2063 6 5022893 6 73938 5000 7 0 1 0.000199362 1 0.353306 0.124825 0.00159744 625.125 0 2063 6 5020564 6 89339 5000 7 0 1 0.000199362 1 0.353306 0.124825 0.00159744 625.125 0 7 6 5018459 6 156100 1150 1283 388 2187 0.0804323 0.509361 0.348864 0.121706 1.17649e-138 609.504 0 7 6 5018691 6 156100 861 811 677 2659 0.0693918 0.339199 0.31898 0.101748 1.10017e-109 509.554 0 7 6 5018910 6 156100 861 811 677 2659 0.0693918 0.339199 0.31898 0.101748 1.10017e-109 509.554 0 Slicing matches in B string 1 tomahawk view -i 1kgp3_chr6_1_45.two -H -I 6 :5e6-6e6,6:5e6-6e6 | head -n 6 FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D DPrime R R2 P ChiSqModel ChiSqTable 3 6 5000012 6 5073477 4988 9 5 6 0.0011915 0.544089 0.465743 0.216917 1.05037e-13 1086.32 0 1027 6 5000160 6 5072168 5000 2 4 2 0.000398404 0.4994 0.407677 0.166201 7.1708e-06 832.333 0 1027 6 5000160 6 5078340 5000 2 4 2 0.000398404 0.4994 0.407677 0.166201 7.1708e-06 832.333 0 3 6 5000482 6 5079621 4993 6 2 7 0.0013931 0.777199 0.64641 0.417846 3.9492e-18 2092.57 0 3 6 5000482 6 5082227 4994 6 1 7 0.00139362 0.874675 0.685808 0.470333 8.78523e-19 2355.43 0 As alluded to in the sorting section , sorted files have generally much faster query times. We can demonstrate this difference by using the sorted and unsorted data from the sliding window example above. 1 2 3 4 5 $ time tomahawk view -i 1kgp3_chr6_4mb.two -H -I 6 :1e6-5e6,6:1e6-5e6 > /dev/null real 3m1.029s user 2m41.929s sys 0m5.682s Sorted 1 2 3 4 5 $ time tomahawk view -i 1kgp3_chr6_4mb_sorted.two -H -I 6 :1e6-5e6,6:1e6-5e6 > /dev/null real 0m38.167s user 0m37.579s sys 0m0.192s Viewing ld data from the binary two file format and filtering out lines with a Fisher's exact test P-value < 1e-4, minor haplotype frequency < 5 and have both markers on the same contig (bit 2 ) 1 tomahawk view -i file.two -P 1e-4 -a 5 -f 2 Example output FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D Dprime R R2 P ChiSqModel ChiSqTable 15 20 1314874 20 2000219 5002 5 0 1 0.00019944127 1 0.4080444 0.16650023 0.0011980831 0 833.83313 15 20 1315271 20 1992301 5005 2 0 1 0.00019956089 1 0.57723492 0.33320019 0.00059904153 0 1668.6665 15 20 1315527 20 1991024 5004 0 3 1 0.00019952102 1 0.49985018 0.2498502 0.00079872204 0 1251.2498 15 20 1315763 20 1982489 5006 0 1 1 0.00019960076 1 0.70703614 0.49990013 0.00039936102 0 2503.4999 15 20 1315807 20 1982446 5004 3 0 1 0.00019952102 1 0.49985018 0.2498502 0.00079872204 0 1251.2498 Example unphased output. Notice that estimated haplotype counts are now floating values and the bit-flag 1 is not set. FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D Dprime R R2 P ChiSqModel ChiSqTable 46 20 1882564 20 1306588 5003.9996 2.0003999 1.0003999 0.99960008 0.00019936141 0.49950022 0.40779945 0.1663004 0.0011978438 0.0011996795 832.83241 46 20 1895185 20 1306588 5004.9998 1.0001999 1.0001999 0.99980011 0.0001994811 0.49970025 0.49970022 0.24970032 0.00079864228 0.00069960005 1250.4992 46 20 1306588 20 1901581 5003.9996 2.0003999 1.0003999 0.99960008 0.00019936141 0.49950022 0.40779945 0.1663004 0.0011978438 0.0011996795 832.83241 46 20 1901581 20 1306588 5003.9996 2.0003999 1.0003999 0.99960008 0.00019936141 0.49950022 0.40779945 0.1663004 0.0011978438 0.0011996795 832.83241 46 20 1306649 20 1885268 5006 1 1.2656777e-08 0.99999999 0.00019960076 1 0.70703614 0.49990013 0.00039936102 0.00039969285 2503.4999 Example output for forced phased math in fast mode. Note that only the REF_REF count is available, the fast math bit-flag is set, and all P and Chi-squared CV values are 0. FLAG CHROM_A POS_A CHROM_B POS_B REF_REF REF_ALT ALT_REF ALT_ALT D Dprime R R2 P ChiSqModel ChiSqTable 67 20 1345922 20 1363176 4928 0 0 0 0.011788686 0.98334378 0.86251211 0.74392718 0 0 0 67 20 1345922 20 1367160 4933 0 0 0 0.011800847 0.98336071 0.89164203 0.79502547 0 0 0 67 20 1345958 20 1348347 4944 0 0 0 0.0092644105 0.97890127 0.85316211 0.72788554 0 0 0 67 20 1345958 20 1354524 4938 0 0 0 0.0092493389 0.86871892 0.80354655 0.64568704 0 0 0 75 20 1345958 20 1356626 4945 0 0 0 0.0033518653 0.99999994 0.51706308 0.26735422 0 0 0","title":"Viewing and manipulating output data"},{"location":"tutorial/#aggregating-and-visualizing-datasets","text":"Tomahawk generally output many millions to many hundreds of millions to billions of output linkage disequilibrium (LD) associations generated from many millions of input SNVs. It is technically very challenging to visualize such large datasets. Read more about aggregation in Tomahawk. Aggregation Description R Pearson correlation coefficient R2 Squared pearson correlation coefficient D Coefficient of linkage disequilibrium Dprime Scaled coefficient of linkage disequilibrium Dp Alias for dprime P Fisher's exact test P-value of the 2x2 haplotype contigency table Hets Number of (0,1) or (1,0) associations Alts Number of (1,1) associations Het Alias for hets Alt Alias for alts Reduction Description Mean Mean number of aggregate Max Largest number in aggregate bin Min Smallest number in aggregate bin Count Total number of records in a bin N Alias for count Total Sum total of aggregated number in a bin In this example, we will aggregate the genome-wide data generated from the sliding example above by aggregating on R2 values and reducing into count in a (4000,4000)-bin space. If a bin have less than 50 observations ( -c ) we will drop all the value and report 0 for that bin. 1 twk aggregate -i 1kgp3_chr6_4mb_sorted.two -x 4000 -y 4000 -f r2 -r count -c 50 -t 4 -O b -o 1kgp3_chr6_4mb_aggregate.twa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Program: tomahawk-7f8eef9b-dirty (Tools for computing, querying and storing LD data) Libraries: tomahawk-0.7.0; ZSTD-1.3.8; htslib 1.9 Contact: Marcus D. R. Klarqvist <mk819@cam.ac.uk> Documentation: https://github.com/mklarqvist/tomahawk License: MIT ---------- [2019-01-25 14:55:43,299][LOG] Calling aggregate... [2019-01-25 14:55:43,299][LOG] Performing 2-pass over data... [2019-01-25 14:55:43,299][LOG] ===== First pass (peeking at landscape) ===== [2019-01-25 14:55:43,299][LOG] Blocks: 47,352 [2019-01-25 14:55:43,299][LOG] Uncompressed size: 50.192950 Gb [2019-01-25 14:55:43,299][LOG][THREAD] Data/thread: 12.548238 Gb [2019-01-25 14:55:43,299][PROGRESS] Time elapsed Variants Progress Est. Time left [2019-01-25 14:56:13,299][PROGRESS] 30,000s 344,930,000 72.8446% 11s [2019-01-25 14:56:27,979][PROGRESS] 44,679s 473,514,826 (10,597,935 variants/s) [2019-01-25 14:56:27,979][PROGRESS] Finished! [2019-01-25 14:56:27,979][LOG] ===== Second pass (building matrix) ===== [2019-01-25 14:56:27,979][LOG] Aggregating 473,514,826 records... [2019-01-25 14:56:27,979][LOG][THREAD] Allocating: 2.560000 Gb for matrices... [2019-01-25 14:56:27,979][PROGRESS] Time elapsed Variants Progress Est. Time left [2019-01-25 14:56:57,979][PROGRESS] 30,000s 348,560,000 73.6112% 10s [2019-01-25 14:57:12,499][PROGRESS] 44,520s 473,514,826 (10,635,926 variants/s) [2019-01-25 14:57:12,499][PROGRESS] Finished! [2019-01-25 14:57:13,713][LOG] Aggregated 473,514,826 records in 16,000,000 bins. [2019-01-25 14:57:13,713][LOG] Finished. You can now use the output binary format .twa in your downstream analysis. Alternatively, it is possible to output a human-readable (x,y)-matrix by setting the -O parameter to u . This tab-delimited matrix can now be loaded in any programming language and used as input for graphical visualizations or for analysis. It is easiest to use these files directly in rtomahawk , the R-bindings for tomahawk .","title":"Aggregating and visualizing datasets"},{"location":"cli/cli-aggregate/","text":"CLI: aggregate \u00b6 Short Synopsis \u00b6 Aggregate TWO data into a rasterized matrix of size [x,y] for plotting. Examples \u00b6 1 tomahawk aggregate -f r2 -r mean -c 50 -i input.twk Options \u00b6 1 2 3 4 5 6 7 8 Options: -i FILE input TWO file ( required ) -x,y INT number of X/Y-axis bins ( default: 1000 ) -f STRING aggregation function : can be one of ( r2,r,d,dprime,dp,p,hets,alts,het,alt )( required ) -r STRING reduction function : can be one of ( mean,count,n,min,max,sd )( required ) -I STRING filter interval <contig>:pos-pos ( TWK/TWO ) or linked interval <contig>:pos-pos,<contig>:pos-pos -c INT min cut-off value used in reduction function : value < c will be set to 0 ( default: 5 ) -t INT number of parallel threads: each thread will use 40 ( x*y ) bytes","title":"aggregate"},{"location":"cli/cli-aggregate/#cli-aggregate","text":"Short","title":"CLI: aggregate"},{"location":"cli/cli-aggregate/#synopsis","text":"Aggregate TWO data into a rasterized matrix of size [x,y] for plotting.","title":"Synopsis"},{"location":"cli/cli-aggregate/#examples","text":"1 tomahawk aggregate -f r2 -r mean -c 50 -i input.twk","title":"Examples"},{"location":"cli/cli-aggregate/#options","text":"1 2 3 4 5 6 7 8 Options: -i FILE input TWO file ( required ) -x,y INT number of X/Y-axis bins ( default: 1000 ) -f STRING aggregation function : can be one of ( r2,r,d,dprime,dp,p,hets,alts,het,alt )( required ) -r STRING reduction function : can be one of ( mean,count,n,min,max,sd )( required ) -I STRING filter interval <contig>:pos-pos ( TWK/TWO ) or linked interval <contig>:pos-pos,<contig>:pos-pos -c INT min cut-off value used in reduction function : value < c will be set to 0 ( default: 5 ) -t INT number of parallel threads: each thread will use 40 ( x*y ) bytes","title":"Options"}]}